[{"categories":["IoT"],"content":" Verify Azure IoT MQ Broker at the Edge Since you are Owner on the edge machine + Azure, you can confirm the broker and its connectivity from both ends. a) Check in IoT Operations Portal (GUI) Go to your instance (Sites → Instances → Your Edge Instance). In the left menu, select Data flow endpoints. You should see one called MQTT (Default) or similar. This is the built-in broker endpoint. Click into it and check: Hostname / IP → where clients should connect. Port(s) → usually 8883 (TLS) and possibly 1883 (non-TLS). Provisioning state → should be Succeeded. Optional: under Activity logs, you can see connection attempts and provisioning messages for that endpoint. Verify \u0026 Test Azure IoT MQ (Broker) from the Edge (K3s) Why netstat doesn’t show port 18883: the broker is exposed as a Kubernetes ClusterIP service (internal only), not a host socket. Use kubectl to verify and test. 1) Verify pods and services bash # Namespace NS=azure-iot-operations # Pods (broker components must be Running) kubectl get pods -n $NS # Broker services (expect ClusterIP on 18883/TCP) kubectl get svc -n $NS | grep -E 'aio-broker|broker' kubectl get svc aio-broker -n $NS -o wide kubectl get endpoints aio-broker -n $NS -o wide You should see aio-broker with PORT(S) 18883/TCP and a backing endpoint like 10.x.x.x:18883. 2) Start a long-running tools pod (for client tests) bash kubectl run -n $NS mqtt-tools \\ --restart=Never --image=alpine:3.19 \\ --command -- sh -c \"sleep infinity\" kubectl -n $NS wait --for=condition=Ready pod/mqtt-tools --timeout=90s # Install test tools inside the pod kubectl -n $NS exec -it mqtt-tools -- sh -lc 'apk add --no-cache mosquitto-clients openssl ca-certificates' 3) Get the broker’s server name and trust chain The broker’s cert must match your hostname and chain to a CA you trust. bash # Inspect the server certificate from its TLS secret (optional) kubectl -n $NS get secret aio-broker-frontend-server-18883 \\ -o jsonpath='{.data.tls\\.crt}' | base64 -d \u003e /tmp/broker-server.crt openssl x509 -in /tmp/broker-server.crt -noout -text | egrep -A2 'Subject:|DNS:' # SANs usually include: # aio-broker, aio-broker.azure-iot-operations, aio-broker.azure-iot-operations.svc.cluster.local Now pull the presented chain from the live broker and extract the root CA it uses: bash # Inside the tools pod: capture presented certs kubectl -n $NS exec -it mqtt-tools -- sh -lc ' echo | openssl s_client -connect aio-broker.azure-iot-operations:18883 \\ -servername aio-broker.azure-iot-operations -showcerts 2\u003e/dev/null \\ | awk \"/-----BEGIN CERTIFICATE-----/{i++} {print \u003e (\\\"/tmp/peer-cert-\\\" i \\\".pem\\\") } END{ print i, \\\"certs saved\\\" }\" for f in /tmp/peer-cert-*.pem; do echo \"== $f ==\"; openssl x509 -in \"$f\" -noout -subject -issuer; done' You should see something like: /tmp/peer-cert-1.pem — server leaf: CN = aio-broker-frontend-server-18883 /tmp/peer-cert-2.pem — self-signed root: CN = Azure IoT Operations Quickstart Root CA - Not for Production Save that root CA to a fixed filename: bash # Copy the presented root CA to a known path inside the pod kubectl -n $NS exec -it mqtt-tools -- sh -lc 'cp /tmp/peer-cert-2.pem /tmp/aio-broker-root-ca.pem' # Sanity check: TLS must verify with this CA kubectl -n $NS exec -it mqtt-tools -- sh -lc ' echo | openssl s_client -connect aio-broker.azure-iot-operations:18883 \\ -servername aio-broker.azure-iot-operations \\ -CAfile /tmp/aio-broker-root-ca.pem -verify_return_error 2\u003e/dev/null \\ | grep \"Verify return code\"' # Expect: Verify return code: 0 (ok) If you see Verify return code: 19 (self-signed certificate in certificate chain), you’re using the wrong CA file. Use the presented root (peer-cert-2.pem) as shown above. 4) Create a Kubernetes Service Account Token (audience aio-internal) bash kubectl -n $NS create token default --audience=aio-internal --duration=1h \u003e /tmp/aio-token.txt kubectl -n $NS cp /tmp/aio-token.txt mqtt-tools:/tmp/aio-token.txt 5) Publish/subscribe using MQTT v5 Enhanced Auth (K8S-SAT) Do not use -u/","date":"15 May, 2025","objectID":"/posts/azure-iot/:0:0","series":null,"tags":["IoT"],"title":"Azure IoT","uri":"/posts/azure-iot/#"},{"categories":["Azure Edge Devices"],"content":" bash export KUBECONFIG=~/.kube/config sysadmin@otedge001:~$ kubectl get namespace NAME STATUS AGE arc-workload-identity Active 3d23h azure-arc Active 3d23h azure-arc-release Active 3d23h azuremonitor-containers Active 2d15h default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d Node Red An overview of the objectives here Local network devices (outside Kubernetes) are sending: MQTT messages Modbus TCP traffic Your Node-RED, running inside K3s, must receive: NodeRED Web Interfaces MQTT subscriptions from devices Modbus polling/requests from devices Then Node-RED forwards processed data to: Azure MQ Service inside the same K3s cluster Expose Node-RED to your local network by: NodePort service to open specific ports from the edge node (otedge001) to the outside world Name Space Create the Name Space bash sysadmin@otedge001:~$ kubectl create namespace nodered namespace/nodered created Presistant Storage Add a PVC (assumes you have default storage class like local-path from K3s) with the following manifest node-red-pvc.yaml: yaml --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nodered-pvc namespace: nodered spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi Apply this storage bash kubectl apply -f nodered-pvc.yaml Since you’re running K3s (and unless you changed nything manually), by default K3s installs the **local-path-provisioner** as the storage backend for dynamic PVCs. That means: Your PVC storage will physically be on the node’s local filesystem Specifically under a folder like: /var/lib/rancher/k3s/storage/ The sub-folder names include: The PVC ID (pvc-2c5110d0-82c0-42fa-99e2-fdf653bdb42c) The namespace (nodered) The pvc name (nodered-pvc) example /var/lib/rancher/k3s/storage/pvc-/ Each PersistentVolume (PV) will have its own subdirectory inside there. bash sysadmin@otedge001:~/nodered$ kubectl get pvc -n nodered NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE nodered-pvc Bound pvc-2c5110d0-82c0-42fa-99e2-fdf653bdb42c 1Gi RWO local-path \u003cunset\u003e 5m15s Health Checking A PVC stuck in Pending typically means the storage provisioner can’t fulfill the request. Let me help you troubleshoot this. Check PVC events for specific errors: bash kubectl describe pvc nodered-pvc -n nodered Look at the Events: section at the bottom—it will show why the provisioner is failing. Verify the local-path provisioner is running: bash kubectl get pods -n kube-system | grep local-path You should see a local-path-provisioner pod in Running state. Check the StorageClass exists and is properly configured: bash kubectl get storageclass local-path -o yaml Deployment Create the manifest for the deployment nodered-deployment.yaml yaml apiVersion: apps/v1 kind: Deployment metadata: name: nodered namespace: nodered labels: app: nodered spec: replicas: 1 selector: matchLabels: app: nodered template: metadata: labels: app: nodered spec: securityContext: fsGroup: 1000 # Ensures files are writable by node-red user containers: - name: nodered image: nodered/node-red:4.1.1-22 ports: - containerPort: 1880 - containerPort: 1883 - containerPort: 502 volumeMounts: - name: nodered-data mountPath: /data resources: requests: memory: \"128Mi\" cpu: \"250m\" limits: memory: \"512Mi\" cpu: \"500m\" volumes: - name: nodered-data persistentVolumeClaim: claimName: nodered-pvc initContainers: - name: install-azure-nodes image: node:18 securityContext: runAsUser: 1000 # Run as same user as Node-RED runAsGroup: 1000 command: - sh - -c - | npm install --prefix /data node-red-contrib-azure-iot-hub node-red-contrib-azure-blob-storage volumeMounts: - name: nodered-data mountPath: /data And deploy it yaml kubectl apply -f nodered-deployment.yaml # Check the Deployment is grabbing a container sysadmin@otedge001:~/nodered$ kubectl get pods -n nodered NAME READY STATUS RESTARTS AGE nodered-7dc66dcdb7-hh2sg 1/1 Running 0 91s Inspect the Container to see if it is starting. In our manifest, we added an ","date":"28 Apr, 2025","objectID":"/posts/iot-nodered-on-arc-enabled-edge/:0:0","series":["Azure IoT Operations"],"tags":["Linux","IoT","Docker"],"title":"IoT - NodeRED on Arc-Enabled Edge","uri":"/posts/iot-nodered-on-arc-enabled-edge/#"},{"categories":["Azure Edge Devices","IoT"],"content":"Starting from an Ubuntu server, which has be Arc Enabled, we can use this as a base for the Azure IoT Operations. Arc Enabled Ubuntu Server K3s Kubernetes on Ubuntu (Single or Multi-Node) Installing K3s K3s provides an installation script that is a convenient way to install it as a service on systemd or openrc based systems. This script is available at https://get.k3s.io. To install K3s using this method, connect to the server using ssh and then run: bash sysadmin@iotedge:~$ curl -sfL https://get.k3s.io | sh - [sudo] password for sysadmin: [INFO] Finding release for channel stable [INFO] Using v1.33.5+k3s1 as release [INFO] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.33.5+k3s1/sha256sum-amd64.txt [INFO] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.33.5+k3s1/k3s [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Skipping installation of SELinux RPM [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s.service [INFO] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service. [INFO] Host iptables-save/iptables-restore tools not found [INFO] Host ip6tables-save/ip6tables-restore tools not found [INFO] systemd: Starting k3ss After running this installation: The K3s service will be configured to automatically restart after node reboots or if the process crashes or is killed Additional utilities will be installed, including kubectl, crictl, ctr, k3s-killall.sh, and k3s-uninstall.sh A kubeconfig file will be written to /etc/rancher/k3s/k3s.yaml and the kubectl installed by K3s will automatically use it A single-node server installation is a fully-functional Kubernetes cluster, including all the datastore, control-plane, kubelet, and container runtime components necessary to host workload pods. Optional Additional K3s Nodes It is not necessary to add additional server or agents nodes, but you may want to do so to add additional capacity or redundancy to your cluster. To install additional agent nodes and add them to the cluster, run the installation script with the K3S_URL and K3S_TOKEN environment variables. Here is an example showing how to join an agent: shell curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh - Setting the K3S_URL parameter causes the installer to configure K3s as an agent, instead of a server. The K3s agent will register with the K3s server listening at the supplied URL. The value to use for K3S_TOKEN is stored at /var/lib/rancher/k3s/server/node-token on your server node. Each machine must have a unique hostname. If your machines do not have unique hostnames, pass the K3S_NODE_NAME environment variable and provide a value with a valid and unique hostname for each node. K3s Configuration When you install K3s on Ubuntu using the quick install script, it places the Kubernetes configuration file at /etc/rancher/k3s/k3s.yaml, which is only accessible by the root user. To use kubectl as a non-root user, it’s best to create your own user-level kubeconfig file. The following commands will create a .kube directory in your home folder, merge the system kubeconfig with your local config, and generate a flattened, self-contained kubeconfig that you can safely use without needing root access. This setup is particularly useful when you’re managing clusters through automation or integrating with tools like Azure Arc, Helm, or GitOps pipelines. bash # Create the .kube directory in your home folder if it doesn't exist mkdir -p ~/.kube # Merge the root-level K3s kubeconfig with your u","date":"24 Apr, 2025","objectID":"/posts/iot-azure-arc-connected-k3s/:0:0","series":["Azure IoT Operations"],"tags":["IaC","Linux","Docker","IoT"],"title":"IoT - Azure Arc Connected K3s ","uri":"/posts/iot-azure-arc-connected-k3s/#"},{"categories":["Infrastructre As Code"],"content":"The integration of artificial intelligence (AI) into cloud infrastructure is revolutionizing the way we manage and deploy cloud resources. AI is no longer just a tool for data analysis; it has become a pivotal component in optimizing the entire lifecycle of cloud architecture. With AI, tasks like deployment, maintenance, and scaling can be done smarter and faster. This article explores the role of AI in empowering cloud architecture and deployments, highlighting the benefits of integrating AI with Infrastructure as Code (IaC) tools like Bicep and Terraform. Infrastructure as Code (IaC) Tools Infrastructure as Code (IaC) tools like Bicep and Terraform are game changers in cloud infrastructure management. These tools simplify infrastructure management by enabling version control, reproducibility, and automation of infrastructure deployment. Here’s what makes them stand out: Bicep Bicep is designed for Azure Resource Manager (ARM) templates, offering a more concise and readable syntax for describing Azure infrastructure. It is ideal for Azure-specific deployments, providing a more streamlined and efficient way to manage Azure resources. Terraform Terraform supports multiple cloud providers, including AWS, Azure, and Google Cloud Platform, making it ideal for multi-cloud scenarios. It maintains a state file to track the current state of the infrastructure, allowing for efficient updates, drift detection, and collaboration among team members. Benefits of AI-IaC Integration Integrating AI with IaC tools takes infrastructure management a step further: Improved Deployment Efficiency AI-assisted deployment ensures that resources are allocated efficiently, reducing the time and effort required for deployment. Peer Programming Generative AI-assisted tools in the Development Environment enable faster scaffolding of solutions and virtual assistant to support complex configuration and debugging scenarios . Reduced Downtime Proactive maintenance enabled by AI reduces downtime, ensuring that resources are always available when needed. Administrative Workload Reduction AI automates repetitive tasks, freeing up administrators to focus on higher-level tasks. Enhanced Performance and Cost Savings AI-optimized resource allocation ensures that resources are utilized efficiently, leading to cost savings and improved performance. Fundamentals Infrastructure as Code Tools Using Infrastructure as Code (IaC) tools like Bicep and Terraform can make a significant difference in cloud architecture and deployment. These tools enable you to: Manage Infrastructure as Code: IaC tools allow you to manage your infrastructure as code, making it easier to version control and reproduce your infrastructure. Version Control and Reproduce: IaC tools enable version control and reproducibility of your infrastructure, ensuring consistency and reliability. Integrate AI Technologies: IaC tools can integrate AI technologies, enabling proactive maintenance, data analysis, and resource optimization. Bicep vs. Terraform Bicep and Terraform are both powerful tools for managing cloud infrastructure. Bicep is designed for Azure-specific deployments, while Terraform supports multiple cloud providers. AI Integration Integrating AI into your cloud infrastructure provides numerous benefits: Data Analysis: AI analyzes vast amounts of data to identify patterns and anomalies, enabling proactive maintenance and resource optimization. Proactive Maintenance: AI-assisted proactive maintenance reduces downtime, ensuring that resources are always available when needed. Resource Optimization: AI optimizes resource allocation, ensuring that resources are utilized efficiently and cost-effectively. CI/CD Tools CI/CD tools are essential for automating testing, validation, and deployment. They speed up deployment and ensure consistency and reliability. Advanced Aspects Efficient Resource Utilization AI-assisted resource allocation ensures that resources are utilized efficiently, leading to cost savi","date":"31 Jul, 2024","objectID":"/posts/using-ai-to-empower-cloud-architecture-and-deployments/:0:0","series":null,"tags":["Bicep","ARM","Terraform"],"title":"Using AI to Empower Cloud Architecture and Deployments","uri":"/posts/using-ai-to-empower-cloud-architecture-and-deployments/#"},{"categories":null,"content":" Evangelist, Thinker, Creative Doer Making Incredible Technology Incredibly Simple! Principal Azure Cloud Solution Architect, ‘Governed Azure Virtual Datacenter’ Product Owner; working for Innofactor Norway, based in Ballina, Ireland. Specialising on Cloud Governance and Operations Automation, DevOps and Development. I define myself as an evangelist; an entrepreneur \u0026 author with an ideology rooted in business insights, technology exploration, pattern analysis and high energy. I envision, theorize and develop system architecture and strategic business platforms, soaked in storytelling and innovative technology. 30 years of Multinational Enterprise IT Experience, leading Cloud implementations since 2009. Recognized for technology knowledge, my passion to share, and an ability to communicate by Microsoft as an MVP since 2011, and Cisco as a Champion since 2014; You need some consulting experience? Feel free to contact me! ### Microsoft MVP Technology experts who passionately share their knowledge with the community ### Infrastructure Code Technology experts who passionately share their knowledge with the community ![Image](img-42464b08-microsoft-mvp-logo-square.png) Some Interesting Facts About Me When not working; I admit to having a passion for IoT, leveraging my background in PLC and embedded electronics, I have been certified in professional Building automation technologies including //Clipsal, Control 4, Iridium Mobile; I have even delivered some professional home automation implementation’s. For the enthusiasts, I am currently invested in the Home Assistant project; and have previously contributed to projects including CodeCore Elve and Charmed Quark (CQC). My code is shared on Github! If and when there is time to play, then the Model Planes, Helicopters and Drones are ready to make an appearance. My Awesome Story Over the last number of years, I have published some white papers and technical articles, was a contributing author to Petri.com, hosting and participating in many webinars, and co-author of multiple Microsoft Cloud related books, and Official Courseware. However, my real passion is meeting and working with people, which I get to embrace as a regular presenter at many international and community events and conferences. I am Currently focused on Cloud Technologies and the Hybrid Datacentre, Specialising in Azure Governance, and sustainable DevOps; Assisting organization’s on Cloud optimized delivery of Legacy Apps and Microservices, leveraging containers and serverless when possible. MY OWN PROFESSIONAL AND TECHNOLOGY SKILLS Professional Skills 80 ** Project Management** 90 ** Certifications** 90 ** Evangelism \\ Communications** Developer Skills / Platform Technologies Azure Resource Manager 90% Serverless (Azure Functions) 65% DevOps (Azure / GitHub) 80% Containers (Docker) 80% Web Apps (HTML) 75% Language Skills / Software development languages and understanding 80 ** PowerShell** 70 ** C#** 30 ** Python** Developer Skills / Platform Technologies Azure Resource Manager 90% Serverless (Azure Functions) 65% DevOps (Azure / GitHub) 80% Containers (Docker) 80% Web Apps (HTML) 75% Technology Skills / Smart Everything Smart Home (Consumer / Opensource) 90% IoT (Prosumer / Industrial / KNX / Modbus) 85% MY ONLINE PRESENTATIONS My mission is to ensure that as you take the time to listen, I focus on delivering the objective of Making Incredible Technology Incredibly Simple! This indirectly imply’s you you must also have a mission, a guiding northstar. Don’t Limit Your Challanges… Challange Your Limits! Join me as a guide for your exciting journey Mastering Technology. My Journey Lionbridge Technologies SYSTEMS ARCHITECT, CORPORATE IT CORE INFRASTRUCTURE 2008 - 2016 Lead corporate technology teams delivering ‘IT as a Service’. Architect responsible for design and introduction of new technologies and strategic solutions. Mentor, Train and provide technical support to solution owners. Led architecture and design of IT Servi","date":"10 Jul, 2024","objectID":"/about/:0:0","series":null,"tags":null,"title":"About Me","uri":"/about/#"},{"categories":["Infrastructre As Code"],"content":"Deploying infrastructure ARM Templates to Azure, but using Tags and their respective value as the parameter configuration settings In a post earlier, we look at using arm to lookup the value of tags’ at both the Subscription and Resource Level. With Bicep this is much easier to understand. This is the same lab configuration as in the original post, but this time to code should be a lot more readable. powershell // Sample to lookup tag values // Both Subscription and Resource Level @description('The resource ID of the resource we wish to look up a tag from.') param sourceResourceId string = '/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vm/providers/Microsoft.Compute/virtualMachines/p-vm001' // Variables to reference the Tags Resource Providers // Subscription Tags Resource Provider var referenceSubscriptionTagsResourceId = '/subscriptions/${subscription().subscriptionId}/providers/Microsoft.Resources/tags/default' // Resource Tags Resource Provider var referenceResourceTagsResourceId = '${sourceResourceId}/providers/Microsoft.Resources/tags/default' var referenceTagsApi = '2020-06-01' // // Lookup the tags and return the value // // Subscription Tags output subscriptionRecoveryVaultRGTag string = reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG output subscriptionRecoveryVaultTag string = reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault // Resource Tags output resourceRecoveryPolicyTag string = reference(referenceResourceTagsResourceId, referenceTagsApi).tags.recoveryPolicy // Concatanation of the outputs to build Resource Ids output recoveryVaultId string = resourceId(subscription().subscriptionId, reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG, 'Microsoft.RecoveryServices/vaults', reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault) output recoveryPolicyId string = '${resourceId(subscription().subscriptionId, reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG, 'Microsoft.RecoveryServices/vaults', reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault)}/backupPolicies/${reference(referenceResourceTagsResourceId, referenceTagsApi).tags.recoveryPolicy}' ","date":"23 Jul, 2021","objectID":"/posts/bicep-tags-as-parameters/:0:0","series":["Building Azure with Code"],"tags":["IaC","ARM","Bicep"],"title":"Bicep - Tags as Parameters","uri":"/posts/bicep-tags-as-parameters/#"},{"categories":null,"content":"Mixup some Vyinl Oldies, a little Ice to cast, and a PI for some energy, and your ready to go A little known trivia - I was once a Disc Jokey, and spent a lot of my youth behind the decks, in clubs around the West Of Ireland. Today, I still am the proud owner of a very large collection of Vynil and CD music, which of course deserves to get a second life with my digital streaming audio system powered by Sonos USB Turntable Streamer I own a really nice turntable which is modeled on the Legendary Technical SL1200 MK3, which I am so well aquatinted with, including the awesome Citronix DJ Console which was home to 2 of these beauties in so many clubs way back when… My Audio-Technica AT-LP120-USB device is the focus of todays IoT challange, I will be using a Raspberry PI3, to stream audio from one of these turntables with USB audio codec output. If your in the market, these are also workable options for this exercise Audio-Technica AT-LP60-USB ION Audio Classic LP | 3-Speed USB Sony PSLX300USB Enable SSH before booting Because we are going to use the Raspberry Pi headless (without a display) and without keyboard attached, we need a way to control the device. Luckily we can enable SSH by adding an empty file called ssh to the root of the SD card. This will enable SSH for us automatically. If you are using the Ethernet port on the Raspberry Pi, networking and SSH should work out of the box with DHCP. Update to Current OS Update your Raspbian install: bash sudo apt-get update Connect the USB Turntable to the Raspberry Pi Now, connect the turntable to the Raspberry Pi, using USB. You can use the command arecord -l to check if your device has been detected. Mine shows this: bash pi@raspberrypi:~ $ arecord -l **** List of CAPTURE Hardware Devices **** card 1: CODEC [USB AUDIO CODEC], device 0: USB Audio [USB Audio] Subdevices: 0/1 Subdevice #0: subdevice #0 Make a note of the card number, 1 in my case. This is probably the same for you, but if it differs, you may need to remember it and change accordingly in the following steps. Fix volume issues As most USB turntables do not have hardware volume control, and the input volume is stuck on roughly half of what it should be, we need to add a software volume control. Create the file /etc/asound.conf and edit it to add the following contents: json pcm.dmic_hw { type hw card 1 channels 2 format S16_LE } pcm.dmic_mm { type mmap_emul slave.pcm dmic_hw } pcm.dmic_sv { type softvol slave.pcm dmic_hw control { name \"Boost Capture Volume\" card 1 } min_dB -5.0 max_dB 20.0 } Next, run this command to refresh the alsa state and also show VU Meters to test the input volume: bash arecord -D dmic_sv -r 44100 -f S16_LE -c 2 --vumeter=stereo /dev/null As you might notice, the volume is way too low. You can use alsamixer to change the volume. Press F6 to select the USB Turntable device, and press TAB until you see the boost slider. I have it set to 65 on my setup, but you might try out. Make sure you are not turning it up too high, or your sound quality might degrade due to clipping. icecast2 Now, to stream we will use the icecast2 package, which of course needs to be deployed to our Pi. bash sudo apt-get install icecast2 Configure icecast2 Next, we will edit /etc/icecast2/icecast.xml, which is the casting servers settings, to provide a name for the stream, and some credential’s to protect the stream. xml \u003cicecast\u003e \u003clocation\u003eRecord Room\u003c/location\u003e \u003cadmin\u003eicemaster@localhost\u003c/admin\u003e \u003climits\u003e \u003cclients\u003e100\u003c/clients\u003e \u003csources\u003e2\u003c/sources\u003e \u003cqueue-size\u003e524288\u003c/queue-size\u003e \u003cclient-timeout\u003e30\u003c/client-timeout\u003e \u003cheader-timeout\u003e15\u003c/header-timeout\u003e \u003csource-timeout\u003e10\u003c/source-timeout\u003e \u003cburst-on-connect\u003e0\u003c/burst-on-connect\u003e \u003cburst-size\u003e65535\u003c/burst-size\u003e \u003c/limits\u003e \u003cauthentication\u003e \u003csource-password\u003evynil\u003c/source-password\u003e \u003crelay-password\u003evynil\u003c/relay-password\u003e \u003c!-- Admin logs in with the username given below --\u003e \u003cadmin-user\u003eadmin\u003c/admin-user\u003e \u003cadmin-password\u003evynil\u003c/admin-password\u003e \u003c/authentication\u003e \u003chostname\u003elocalhost\u003c/hostn","date":"21 Feb, 2021","objectID":"/posts/streaming-vinyl-on-sonos/:0:0","series":null,"tags":["Linux"],"title":"Streaming Vinyl On Sonos","uri":"/posts/streaming-vinyl-on-sonos/#"},{"categories":["Infrastructre As Code"],"content":"Deploying infrastructure ARM Templates to Azure, but using Tags and thier respective value as the parameter configuration settings In the post, I am going to introduce a concept which will allow you to greatly up your Infrastructure as Code game, by using Azure as a State Machine! One of the typical challenges when deploying ARM templates, is the sheer number of parameters which we find as a requirement to complete a deployment; which as you will appreciate gets considerably harder as we target many environments. There are a number of methods to address this, including the use of Parameter files or Continuous deployment variables; each with their own challenges. Resource Tags Tags can be applied at both the level of the subscription and resources. For the purpose of this post we will use a scenario of the recovery vault. At the subscription level, we will apply 2 tags to identity the vault we will target for the resources, and then on the actual resource we will add a tag to identity the recovery policy we wish to be applied. Subscription Level Tags With these Tags applied, we will have something similar to the following at the subscription level: In my lab the subscription is /subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535 Resource Tags And the VM called p-vm001, which will represent the resource which we are going to monitor the tag on in my lab will be in the same subscription (for permissions to be simplified), hosted in a resource group called p-vm The full resource ID of this VM in the lab is /subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vm/providers/Microsoft.Compute/virtualMachines/p-vm001 ARM Magic Now, we have all the parts of this environment in place, we will create an ARM template, which simply looks up the values of these tags for us, and to illustrate how it works, it will return the values as outputs. Using the Custom Template Deployment in Azure Portal, paste the sample template below json { \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"sourceResourceId\": { \"type\": \"String\", \"defaultValue\": \"/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vm/providers/Microsoft.Compute/virtualMachines/p-vm001\", \"metadata\": { \"description\": \"The resource ID of the resource we wish to look up a tag from.\" } } }, \"variables\": { \"referenceSubscriptionTagsResourceId\": \"[concat('/subscriptions/', subscription().subscriptionId, '/providers/Microsoft.Resources/tags/default')]\", \"referenceResourceTagsResourceId\": \"[concat(parameters('sourceResourceId'),'/providers/Microsoft.Resources/tags/default')]\", \"referenceTagsApi\": \"2020-06-01\" }, \"resources\": [ ], \"outputs\": { \"recoveryVaultId\": { \"type\": \"String\", \"value\": \"[resourceId(subscription().subscriptionId, reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG ,'Microsoft.RecoveryServices/vaults', reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault) ]\" }, \"recoveryPolicyId\": { \"type\": \"string\", \"value\": \"[concat( resourceId(subscription().subscriptionId, reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG ,'Microsoft.RecoveryServices/vaults', reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault), '/backupPolicies/', reference(variables('referenceResourceTagsResourceId'), variables('referenceTagsApi')).tags.recoveryPolicy )]\" }, \"subscriptionRecoveryVaultRGTag\": { \"type\": \"string\", \"value\": \"[reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG]\" }, \"subscriptionRecoveryVaultTag\": { \"type\": \"string\", \"value\": \"[reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault]\" }, \"resourceRecoveryPolicyTag\": { \"type\"","date":"17 Jan, 2021","objectID":"/posts/azure-iac-tags-as-parameters/:0:0","series":null,"tags":["ARM","IaC"],"title":"Azure IaC - Tags as Parameters","uri":"/posts/azure-iac-tags-as-parameters/#"},{"categories":null,"content":"Oxidized is a Linux based service which has the ability to monitor a device’s configuration, including software and hardware. Current configuration is backed up from each device and stored to a GIT repository to maintain history of changes. The process is very simple: Login to each device in the router list router.db, Run Commands to get the information that will be saved Clean the output Commit the Changes to GIT Repository The tool is coded in Ruby, and implements a Domain Specific Language (DSL) for interaction. Finally, there is a Web based User experience included in the solution so we can get a fast overview of the world. Docker Container All of the configuration for my container is hosted at the file system location /opt/appdata/oxidized I will also select to execute the Web Interface for Oxidized using its default port with is tcp:8888 Using the follow command, we will grab the latest container version from Docker Hub, and call the container oxidized locally. Additionally, if the container should stop, I am providing the flag to instruct docker to always restart the service again. bash sudo docker run --restart always -v /opt/appdata/oxidized:/root/.config/oxidized -p 8888:8888/tcp -t oxidized/oxidized:latest oxidized Configuration We need a configuration file to guide Oxidized running process bash vi config The following is the configuration sample that I am running with yaml --- username: admin password: P@ssw0rd! model: junos resolve_dns: true interval: 3600 use_syslog: false debug: false threads: 30 timeout: 20 retries: 3 prompt: !ruby/regexp /^([\\w.@-]+[#\u003e]\\s?)$/ rest: 0.0.0.0:8888 next_adds_job: false vars: {} groups: {} models: {} pid: \"/root/.config/oxidized/pid\" crash: directory: \"/root/.config/oxidized/crashes\" hostnames: false stats: history_size: 10 input: default: ssh, telnet debug: false ssh: secure: false ftp: passive: true utf8_encoded: true output: default: git file: directory: \"/root/.config/oxidized/configs\" git: single_repo: true user: Oxidized email: oxidized@email.target repo: \"~/.config/oxidized/oxidized.git\" source: default: csv csv: file: ~/.config/oxidized/router.db delimiter: !ruby/regexp /:/ map: name: 0 ip: 1 model: 2 username: 3 password: 4 model_map: cisco: ios juniper: junos unifiap: airos edgeos: edgeos Device list The table based on the configuration we just defined, will be formatted as follows To populate the table, we can open the editor vi router.db, and then inset the following sample entries yaml Bedroom1_ap:172.16.1.114:unifiap:sysadmin:P@ssw0rd! Kitchen_ap:172.16.1.121:unifiap:sysadmin:P@ssw0rd! Cinema_ap:172.16.1.160:unifiap:sysadmin:P@ssw0rd! ServerRoom_ap:172.16.1.115:unifiap:sysadmin:P@ssw0rd! Firewall:172.16.1.1:edgeos:ubnt:Sc0rp10n! Now, we are ready, we have the configuration all set for this installation Web Interface Launching our browser to the oxidized site hosted on TCP 8888 renders the current status From here we can see all the version changes for the devices configuration And even select any one of these change sets, and view the changes which were applied to the configuration Closing Thoughts Now, How do you think this might work with Azure?… ","date":"29 Jun, 2020","objectID":"/posts/change-detection-using-oxidized/:0:0","series":null,"tags":["Linux"],"title":"Change Detection Using Oxidized","uri":"/posts/change-detection-using-oxidized/#"},{"categories":["Azure Management"],"content":"Delegating Azure Enterprise Agreement Owner privileges to a Service Principal (SPN) Under the Enterprise agreement we have some different Persona’s, which have quite different abilities and operations upon which they are permitted to preform. Before we being the process of delegation, It is important to understand this Hierarchy, so we can correctly proceed with the technical work ahead. Enterprise Administrator Has the ability to add additional Enterprise and Department Administrators, Additional this persona can Add Departments Add or Associate Accounts to and Enrolment Can view usage and charges across ALL Accounts and Subscriptions Can view the monetary commitment balance associated to the Enrolment There is no limit on the number of Enterprise Administrators that can be associated with an Enrolment, and additionally a notification contact can be assigned to receive all email notifications issued. Department Administrator The Department Administrator has ability to do the following: Create Department Administrator (Department focus – click on add administrator) View/Edit Department properties such as name or Cost Center (Department focus – click on edit pen icon) Create a new Account Owner on the Department they administer (Switch to Account focus – click on add account) Remove the associated Accounts from the Department they administer (In Account focus – hover over account and then select the x icon to delete) Download usage details of the Department they administer (Switch to Reports panel on left – Select Download Usage focus) View the monthly Usage and Charges associated to their Department if Enterprise Administrator has granted permission to do so. (Switch to Reports panel on left –Select Usage Summary focus) Enable the Account owners to create Non Production Subscriptions Subscription Offer MS-AZR-0148P for Dev/Test Subscription Offer MS-AZR-0017P for Production Account Owners The Account Owner can add Subscriptions to their Accounts. Additionally they have the ability to Update the Service Administrator for a Subscription View Usage Data for their Account If enabled by the Enterprise Administrator, can also view Account Charges Enumerate existing subscriptions in their account Create new subscriptions within the scope of their account The Account Owner also has the privilege’s of delegating their responsibility (Role) to a Service Principal; which is the core of the process we are going to undertaken in this post. As of March 2020; this is no longer relevant. This account is REQUIRED to have established at least on subscription manually. In the EA Portal, the Account page for a Department will present a list of established subscription. Programmatic Subscription Creation The objective of the is process is to delegate the EA Account Owner privilege’s to a Service Principal, which can be leveraged to programmatically establish subscriptions. While the procedure is not overly complex; we will break the operation in 4 key milestones, which should make this easier to understand. EA Department Account Owner Service Principal for Delegation Delegating EA Account Owner Privilege’s Validation EA Department Account Owner Using the Azure AD account credentials for your Microsoft Tenant, you should be able to authenticate with the EA Portal, and view the Account page, as illustrated earlier. Once you have confirmed that this works as expected, we will repeat this authentication again, however this time, using either the Azure Powershell Module or the Azure CLI; The guide will present the commands used in either scenario. Authenticate to Azure the EA Account Owner Credentials With working credentials validated in the EA Web portal, proceed with a new shell session and authenticate to Azure with your EA Account Owner credentials; you may also take advantage of the Azure Cloud Shell which will automate the login experience. The credentials MUST be hosted in the tenants AAD instance. Microsoft Accounts, or Guest accounts from oth","date":"24 Apr, 2020","objectID":"/posts/azure-enterprise-agreement-delegation/:0:0","series":null,"tags":["Identity"],"title":"Azure Enterprise Agreement Delegation","uri":"/posts/azure-enterprise-agreement-delegation/#"},{"categories":["Infrastructre As Code"],"content":"Retrieve the Function Host Keys while deploying an ARM template Todays conundrum: As I deploy a new Function Application, I need a simple methodology to retrieve the Host Keys for the function application so that I validate the deployment has been successful; and potentially pass on the key to related services, for example API Management. As before, I am leveraging templates, and will stay cloud native; this time depending on the functions Output ability to present the keys. Solution A not well know ARM resource is going to be our Hero in this journey. The resource is a called Microsoft.Web/sites/host/functionKeys and you can gain a little (actually almost none) more details on the Microsoft reference site. The Documentation refer the API release 2018-02-01; But as you will see in the example code; I discovered that a slightly newer version available. The Key to this deployment is the following resource definition json \"resources\": [ { \"comments\": \"~~ Function App Keys ~~\", \"type\": \"Microsoft.Web/sites/host/functionKeys\", \"apiVersion\": \"2018-11-01\", \"name\": \"[concat(variables('webappName'), '/default/apimanagement')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Web/sites/', variables('webappName'))]\" ], \"properties\": { \"name\": \"api-management\" } } ] Let’s inspect this closer After this resource has been deployed, we then will reference this in our template output section json \"outputs\": { \"functionKey\": { \"type\": \"string\", \"value\": \"[listkeys(concat(resourceId('Microsoft.Web/sites', variables('webappName')), '/host/default/'),'2016-08-01').functionKeys.apimanagement]\" } } In the value section of this output, we are getting a reference to the Function Applications resource identity we just deployed, and with this leverage the ARM Function of listkeys to return the function key for the resource we identified on Line 5 in the earlier snippet. With this output, we can now chain this as the input for additional templates, or reference the value for our scripts. ","date":"22 Apr, 2020","objectID":"/posts/azure-iac-function-keys/:0:0","series":["Building Azure with Code"],"tags":["IaC","ARM"],"title":"Azure IaC - Function Keys","uri":"/posts/azure-iac-function-keys/#"},{"categories":["Infrastructre As Code"],"content":"Dynamically appending Tags to our ARM template with the union function Todays conundrum: As I am leveraging templates, there will always be some standard tags I require to implement within the template, but I also require to provide additional tags as a parameter to be appended with the deployment. My objective is to set up tags within an ARM template in accordance with good governance and the Cloud adoption framework. Solution ARM Template functions to the rescue. Todays salvation is called union, which you can learn more about on the actual reference Site This is the existing implementation json { \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"configVersion\": { \"type\": \"string\", \"defaultValue\": \"1.0.0.0\", \"metadata\": { \"description\": \"The version of the parameters file that holds the configuration.\" } }, \"purpose\": { \"type\": \"string\", \"allowedValues\": [ \"diag\", \"log\", \"audit\", \"data\" ], \"defaultValue\": \"data\", \"metadata\": { \"description\": \"The designated purpose of the storage account. 'diag' for diagnostics, 'log' for logging, 'audit' for auditing, 'data' for data storage\" } }, \"resilience\": { \"type\": \"string\", \"allowedValues\": [ \"Standard_LRS\", \"Standard_ZRS\", \"Standard_GRS\", \"Standard_RAGRS\" ], \"defaultValue\": \"Standard_LRS\", \"metadata\": { \"description\": \"Choose a level of resilience and tier suitable for the purpose and region\" } }, \"tier\": { \"type\": \"string\", \"allowedValues\": [ \"Standard\", \"Premium\" ], \"defaultValue\": \"Standard\", \"metadata\": { \"description\": \"Choose tier, Standard or Premium.\" } }, \"kind\": { \"type\": \"string\", \"allowedValues\": [ \"Storage\", \"StorageV2\", \"BlobStorage\", \"FileStorage\", \"BlockBlobStorage\" ], \"defaultValue\": \"StorageV2\", \"metadata\": { \"description\": \"Choose a kind of storage account\" } } }, \"variables\": { \"iacVersion\": \"undefined\", \"rgName\": \"[resourceGroup().name]\", \"rgLocation\": \"[resourceGroup().location]\", \"uniqueString\": \"[uniqueString(subscription().id, resourceGroup().id)]\", \"storageAccountAffix\": \"[concat(replace(variables('rgName'), '-', ''), parameters('purpose'))]\", \"storageAccountName\": \"[toLower(substring(replace(concat(variables('storageAccountAffix'), variables('uniqueString')), '-', ''), 0, 23) )]\" }, \"resources\": [ { \"comments\": \"~~ Storage Account ~~\", \"type\": \"Microsoft.Storage/storageAccounts\", \"apiVersion\": \"2018-07-01\", \"name\": \"[variables('storageAccountName')]\", \"sku\": { \"name\": \"[parameters('resilience')]\", \"tier\": \"[parameters('tier')]\" }, \"kind\": \"[parameters('kind')]\", \"location\": \"[variables('rgLocation')]\", \"tags\": { \"IaCVersion\": \"[variables('iacVersion')]\", \"ConfigVersion\": \"[parameters('configVersion')]\" }, \"scale\": null, \"properties\": { \"supportsHttpsTrafficOnly\": true, \"encryption\": { \"services\": { \"file\": { \"keyType\": \"Account\", \"enabled\": true }, \"blob\": { \"keyType\": \"Account\", \"enabled\": true } }, \"keySource\": \"Microsoft.Storage\" }, \"accessTier\": \"[if(equals(parameters('kind'), 'Storage'), json('null'),'Hot')]\" }, \"dependsOn\": [ ] } ], \"outputs\": { \"storageId\": { \"type\": \"string\", \"value\": \"[resourceId('Microsoft.Storage/storageAccounts', variables('storageAccountName'))]\" }, \"storageAccountName\": { \"type\": \"string\", \"value\": \"[variables('storageAccountName')]\" } } } The Change Set And using our new union we can resolve the puzzle with 3 simple changes To the parameter we add a new object json \"tagValues\": { \"type\": \"object\", \"defaultValue\": { \"Dept\": \"Undefined\", \"Environment\": \"Development\" } }, In the variables we can define our default tags json \"defaultTag\": { \"IaCVersion\": \"[variables('iacVersion')]\", \"ConfigVersion\": \"[parameters('configVersion')]\" }, And in the resource we can use the union function to merge the objects together json \"tags\": \"[union(parameters('tagValues'),variables('defaultTag'))]\", The Final Result The final template will look as follows json { \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\", \"","date":"1 Apr, 2020","objectID":"/posts/azure-iac-appending-tags/:0:0","series":["Building Azure with Code"],"tags":["IaC","ARM","Bicep"],"title":"Azure IaC - Appending Tags","uri":"/posts/azure-iac-appending-tags/#"},{"categories":["Azure Network Connectivity"],"content":"The Journey started with the concept of VNets, with work loads, and have evolved in the direction of Subnets, and quickly became a very complex list of islands which were disconnected Security Public Cloud SaaS, Internet Users Branch Offices Virtual WAN is a managed service Managed by Microsoft with global scale, and multplie endpoints. Each Hub can support 60Gb of connectivity; Including 20Gb of ExpressRoute. 20Gb of User VPN 20Gb Site to Site Supports 10K users per hub, 1000 sites per hub Transit Routing Cloud Network orchestration Automation large scale branch, SDWAN CPE connectivity Overview Simplified networking, ease of user operations, and cost savings: Any-to-Any Connectivity Full mesh hubs Branch to Azure Branch to Branch VPN \u003c-\u003e ExpressRoute User VPN \u003c-\u003e Site vNet to vNet Whats New Any-to-Any connectivity (Preview) Express Route , User VPN (Point to Site) GA ExpressRoute Encryption Multi Link Azure Path Selection Custom IPSec Connect VNG VPN to Virtual WAN Available in Gove Cloud and China Azure Firewall integration (preview) Pricing Virtual WAN Types Basic VPN Only Branch to Azure Branch to Branch Connect VNET DIY VNet Peering (VNet to VNet - no transitive) Standard = Basic + Multi Link Support in VPN Sites Dynamic traffic distribution across ISP at the branch site Express Route (Standard VWan) 20Gb aggregate throughput Private Connectivity Requires Premium Circuit In Global Reach LocationExpressRoute VPN Interconnect ExpressRoute and Site-to-Site/Point-to-Site User VPNExpressRoute to ExpressRoute (Premium) Express Route Encryption IPSec over Express Route (Azure Azure Private IP) User VPN IPSec and OpenVPN support for up to 10K users Azure Firewall Firewall in Virtual Hub Centralised Policy and route managmenet VNET to Inernet via Firewall Branhc to ingtern via the firewall MSP Partner Program Announced in July 2019 - in the Azure Marketplace Pricing Connections, Traffic, Aggregate via the Hubs Connection Unit Site to Site VPN 0.05/hour User VPN 0.03/hour Scale Unit 1 Unit = .361/h 500Mb 1 ER Scale Unit = 0.42/hr 2Gbos Virtual Hub Basic Hub - Free Standard - 0.25/hour Zero Thrust Networking Microsegmention Segment Prevent Lateral Movement and data exfilration Protect Connect Cloud Native Services, all software defined resources implement the Defence in Depth offer, the resources included are: Azure Firewall Azure Web Application Firewall Azure Private Link Azure DDoS Protection Virtual Network Network Security Groups User Defined Routes Load Balancer Network Segmentation Host Based - With agent Installed HyperVistor Baed - VMWare NSX Network Based - Softwaew Defined Networking Subscription Logic isolation of environemtn and all resoruces Virtual Network Isoared and highly secure enviroonment to run virtual machines and applications Network Security Group Enforce and control network traffic securitly rules to allow or deny traiffc fro a vnet or vm Web Application Firewall Application specific network security Azure Firewall ","date":"7 Nov, 2019","objectID":"/posts/global-transit-network-for-azure-virtual-wan/:0:0","series":["Ignite 2019"],"tags":["Conference"],"title":"Global Transit Network For Azure Virtual WAN","uri":"/posts/global-transit-network-for-azure-virtual-wan/#"},{"categories":["Azure Management"],"content":"Magnify the power of extending Azure platform by enabling customers and partners to easily bring in custom solutions to azure. These can be scoped for offering to our own enterprise, or just some selected customers; or even all customers. Challenges with extending azure include many of the typical thoughts we face As part of my deployment i need to do extra works Need to interface with external APIs, create users, storage tables, calling APIs external to Azure, while deploying ARM templates 200 Services, which service should i be selected, What is the correct VM SKU? what would be more cost efficient How do I integrate my service into Azure; What is the correct option to expose my service to my enterprise, or all azure users How do we deploy and offer? Deployment Script New resource type - Microsoft.Resources/DeploymentScripts Allows running any scripts, Azure CLI, Powershell, Content can be provided in line of using a URI Can also execute Pre or Post configuration on ARM resources, Fire and forget resource type, configurable auto-deletion of this, should it be deleted, and if so when? Azure Service Catalog Service Catalog blade: Enable applications development teams to be more successful with the right services and scope the selections offered Used in conjunction with Policy and RBAC, supports the compliance with the organization standard. Customized the creation experience for services and solutions Applications are managed by a Central IT / Support Team Custom Resource Providers Organizations want to extend ARM and Azure management to the services they user, both custom and 3rd party build. Partners want to extend their custom resources directly into Azure for their customers Managed app developers need to give some control to their custom resources, for example integrate a SaaS service in Azure, examples include DataDog or Service Now. Any REST endpoint can be called from the new Custom Provider Endpoint. Any API can be extended into Azure, with RBAC Policy, Activity Log etc. Partners can have full control with governances, policy and templates for these services VSCode Managed Application extension is currently in Private Preview Azure Policy extension allows the ability to link the resource with the managed application, so that each time a new resource is deployed the managed application can process the details. Enabled management access out of the box, Easy access to author, no code needed to extend azure Monotised through the application lifecycle. Managed application has full access to the target subscription when deployed. Partners - Managed Applications Center See, manage, and deploy all instances of the applications which may be deployed, managed as scale across tenants. Cloud Partner portal, new offer under Managed Applications, new SKU. Include the packaged file for the Managed Application, with the tenant ID, and the view definitions. Define who will have access - reader, contributor, etc. - that can cross-tenant manage these services. How to bill, using a private preview (opened in December 2019) using customer metering that can be built on any measure; with price per unit, and amount offered as part of the base prices. Can be up to 18 line items for the application, with different prices by SKUs and users.Private Offers also supported for per-customer pricing. Resource Providers Building a full-blown resource provider in Azure. Most powerful mechanism to deliver your service in Azure. 220+ RPs currently in Azure, first-party account for 90% Expose resource types specific to your service, including CRUD, etc. Get all the benefits of the ARM environment, including RBAC, Policy, etc. Why? Custom use native Azure services AND partner Services. Homogeneous experience across services Capabilities parity across services and custom billing based on Azure billingLeverage Azure Arc to provide capabilities over resources on on-premises resources. Custom views for the Custom resource providers ","date":"7 Nov, 2019","objectID":"/posts/managed-applications-and-custom-resource-providers/:0:0","series":["Ignite 2019"],"tags":["Conference"],"title":"Managed Applications and Custom Resource Providers","uri":"/posts/managed-applications-and-custom-resource-providers/#"},{"categories":["Networking Infrastructure \u0026 Security"],"content":"Delivering PaaS Services Privately on Azure VNets with Private Link Mission Critical HTTP Applications, there are many things to consider Personalised, Micro-Services, Rich Context…. To support this MS have a number of services i the Suite - Azure Frontdoor, Application Gateway, Azure CDN, Web Application Firewall, Azure Load Balancer, and Azure Traffic Manager Azure Application Gateway Regional Gateway as a service Standard V2 SKU in GA, Currently Available in 26 regions, Builtin Zone Redundancy, Static VIP, HTTP Header/cookies insertion/modification Increased scale limits 20 -\u003e 100 Listeners Key Vault integration and auto-renewal of SSL Certs AKS ingress Controller Autoscaling and Performance Improvements Grow and shrink based on app traffic requirements 5X better SSL offloads performance 500-50,000 connections/sec with RSA 2048 bit certs 30,000-3,000,000 persistent connections 2,500-250,000 reqs/sec Announcing General Availability: Frontend TLS cert integration with Azure Key Vault Utilized user-assigned managed identity access control for key vault User Certificates or secrets on key vault Polls every 4 hours to enable automatic cert renewal manual override of specific certificate version retrieval Manipulate Request and Response headers \u0026 cookies Strip port from X-Forwarded-for header Add security headers like HSTS and X-XSS-Protection Common header manipulation ex HOST, SERVER AKS Ingress Control using Application Gateways Deployed using Helm Utilizes Pod-AAD for ARM authentication Tighter integration with AKS add on support coming Support URI path based, host based, SSL termination, SSL re-encryption, redirection, custom health probes, draining, cookie affinity Support for Lets Encrypt provide TLS certificates WAF fully supported with custom listener policy Support for multiple AKS as backend Support for mixed mode - both AKS and other backend types on the same Application Gateway http://aka.ms/appgwaks Wild Card Listener Support for Wildcard characters in the listener host name Support for * and ? Characters in host name Associated wildcard or SAN certificates the service HTTPS enabled domains Send traffic to multiple tenant end points Diagnostics and logs enhancements TLS Protocol TLS Cipher Backend target server backend response code backend latency Metrics Backend response status code RPS healthy nodes End to End Latency Backend Latency Backend connect, first byte and last byte latency App Monitor Insights for Application Gateway Single health and metic console for your entire cloud network No agent/configuration required Azure WAF - Cloud Native WEB Application Firewall Unified WAF offering to protect your apps at network edge or region uniformly Public preview announced Microsoft threats intelligence Protect agains automatic attacks Managed good and bad bots with Azure BotManager Rule Set Data is refreshed daily Easy to configure in WAF policy Helps increase your applications performance, by stopping aggressive crawlers. Site and URI path specific WAF Policies Customized WAF police at the region WAF Assign different Policies to different sites Site specific polices implies you can tune the WAF to suit the needs of each site independently Geo filtering on regional WAF Allow or Block a list of countries, Support log mode Rule Set for CRS 3.1 added (to be the default soon) Integration with Azure Sentinel Performance and concurrency enhancements ","date":"7 Nov, 2019","objectID":"/posts/web-application-gateway/:0:0","series":["Ignite 2019"],"tags":["Conference"],"title":"Web Application Gateway","uri":"/posts/web-application-gateway/#"},{"categories":["Infrastructre As Code"],"content":"Apache Guacamole is a free and open source web application which lets you access your dashboard from anywhere using a modern web browser. It is a clientless remote desktop gateway which only requires Guacamole installed on a server and a web browser supporting HTML5. Guacamole is the best way to keep multiple instances accessible over the internet. Once you add an instance to Guacamole, you don’t need to remember the password as it can securely store the credentials. It also lets you share the desktops among other users in a group. Guacamole supports multiple connection methods such as SSH, Telnet, VNC, and RDP. In this tutorial, we will install Apache Guacamole on a Azure with an Ubuntu 16.04 instance. Guacamole Server Guacamole server consists of the native server-side libraries required to connect to the server and the guacd tool. guacd is the Guacamole proxy daemon which accepts the user’s connections and connects to the remote desktop on their behalf. Given below is the architecture of Guacamole System. Note: It is required to compile and install the Guacamole server on the host machine, installing the binary is not possible for Guacamole server Server Install Download the Guacamole server source code files into the temporary directory. shell cd /tmp wget \"http://apache.org/dyn/closer.cgi?action=download\u0026filename=guacamole/0.9.14/source/guacamole-server-0.9.14.tar.gz\" -O guacamole-server-0.9.14.tar.gz Extract the source code archive. shell tar xf guacamole-server-0.9.*.tar.gz cd guacamole-server-0.9.* Compile and install the source code. shell ./configure --with-init-dir=/etc/init.d makemake install The installation will also set up an init script which can be used to manage the guacd daemon. Create the necessary links and cache for the shared libraries. shell ldconfig Guacamole server is now installed on your instance. Start the Guacamole proxy daemon and enable it to automatically start at boot time using the following commands. shell systemctl enable guacd systemctl start guacd You can check the status of the service by running. shell systemctl status guacd Guacamole Client Guacamole client is Java based web application which contains all the Java and JavaScript code required for running the user interface of Guacamole. It ultimately creates a web application which connects to the guacd daemon running in the background using Guacamole protocol. In the foreground, it renders the remote desktop interface using HTML5 on the web browser to the authorized users. Unlike Guacamole server, Guacamole client is not required to be compiled and install from source. Cross-platform Guacamole client binary is available to download and install. Apache Tomcat Install Guacamole binary requires a Java web server to run. In this tutorial, we will install Apache Tomcat 7 or 8 to run the Guacamole binary file. Install Java 8 runtime on your server, installing JDK is not required since we do not need to compile any Java code. shell yum -y install java-1.8.0-openjdk.x86_64 Create a new group and user for Tomcat installation. Running Tomcat server with an unprivileged user is recommended for security reasons. shell groupadd tomcat useradd -M -s /bin/nologin -g tomcat -d /opt/tomcat tomcat Download latest Tomcat server of version 8.5 from Apache mirror. shell wget http://www-us.apache.org/dist/tomcat/tomcat-8/v8.5.28/bin/apache-tomcat-8.5.28.tar.gz Extract the archive into /opt/tomcat directory. shell mkdir /opt/tomcat tar xvf apache-tomcat-8*.tar.gz -C /opt/tomcat --strip-components=1 Provide appropriate permissions and ownership to Tomcat server files. shell cd /opt/tomcat chgrp -R tomcat /opt/tomcat chmod -R g+r conf chmod g+x conf chown -R tomcat webapps/ work/ temp/ logs/ Create a new systemd service file for managing Tomcat server. shell nano /etc/systemd/system/tomcat.service Populate the file with the following configuration. plain [Unit] Description=Apache Tomcat Web Application Container After=syslog.target network.target [Service] Ty","date":"2 Nov, 2019","objectID":"/posts/guacamole-azure-appliance/:0:0","series":null,"tags":["Linux","ARM"],"title":"Guacamole Azure Appliance","uri":"/posts/guacamole-azure-appliance/#"},{"categories":null,"content":"Turn back to 2007; My wife and I built our home, integrating many smart technologies, including the Clipsal C-Bus lighting system. This solution is classified as a Prosumer technology, and is designed to integrate into whole house automation systems. The C-Bus system implements however a propriatory technology, and utilizes a communication protocol which is not ‘open source’; however, accepting a licence agreement will permit access to this protocol for creating an programming interface. To simplify (arguable) the process of integrating with the CBus environment Clipsal released a Bridge solution which enables a TCP interface using a special Java application called ‘C-GATE’. Using a Raspberry Pi, with a USB to RS-232 cable, which is then connected to a Clipsal interface called the Serial PCI Module, Prerequisites Deploy the current relase of Rasbian for your Pi. Update: Jan 2021 - Currently using Rasbian 2021-01-11 Once the Pi have been configured and added to the network, we can connect via SSH, and begin installing the pre-requisites for our gateway. shell # Serial 2 Socket Build Tools sudo apt-get install git build-essential autotools-dev devscripts libssl-dev # Java 8 Runtime for CGate sudo apt-get installopenjdk-8-jdk # Node and NPM for CGateWeb sudo apt-get installnode npm TCP to Serial Bridge Configuring the CBus system over TCP however will not work with just C-Gate alone, we need to also establish a TCP connection directly to the Serial PCI Module. Serial To Socket This requires that we compile and run a small C application (Don’t worry, this is painless and fast); It took a lot of searching to find, posted the source to the following GIT repository; the application is called ser2sock. Here’s the steps to get it set up: shell git clone https://github.com/nutechsoftware/ser2sock.git cd ser2sock ./configure --without-sslcc -o ser2sock ser2sock.c sudo mv ser2sock /usr/local/bin cd /usr/local/bin/ser2sock sudo chown -R pi:pi /usr/local/bin/ser2sock Running as a service - System.d The source offers us a sample init script which we can use for starting the service. First we will place this in the SystemD folder, and then update it to match our requirements for C-Gate shell sudo cp ~/ser2sock/init/systemd/ser2sock.service /etc/systemd/system Update the startup script, /etc/systemd/system/ser2sock.service to have the daemon auto-start with the required C-Gate port, which is TCP 10001, and the serial interface baud rate set to 9600. shell [Unit] Description=Proxy that allows tcp connections to serial ports After=syslog.target network.target [Service] Type=forking ExecStart=/usr/local/bin/ser2sock -p 10001 -s /dev/ttyUSB0 -b 9600 -d ExecReload=/bin/kill -HUP $MAINPID [Install] WantedBy=multi-user.target Then activate using: shell sudo systemctl enable ser2sock.service sudo systemctl start ser2sock.service Installing C-Gate Download the current software release of C-Gate off the clipsal website and unzipped the files into /usr/local/bin/cgate. shell cd ~ wget https://updates.clipsal.com/ClipsalSoftwareDownload/mainsite/cis/technical/CGate/cgate-2.11.4_3251.zip unzip cgate-*.zip sudo mv cgate /usr/local/bin Running C-Gate as Service - System.d Adding the following ‘system.d’ startup script, to have the daemon auto-start with the operating system /etc/systemd/system/cgate.service shell [Unit] Description=Clipsal CBUS Gateway After=syslog.target network.target[Service] ExecStart=/usr/bin/java -Djava.awt.headless=true -jar -noverify /usr/local/bin/cgate/cgate.jar Restart=always User=root Group=root Environment=PATH=/usr/bin:/usr/local/bin WorkingDirectory=/usr/local/bin/cgate/ [Install] WantedBy=multi-user.target Then activate using shell sudo systemctl enable cgate.service sudo systemctl start cgate.service C-Gate Access Control We must configure the C-Gate service to allow remote connections from machines on the network by editing the access control file nano /usr/local/bin/cgate/config/access.txt; adding a line, providing the ip ","date":"10 Oct, 2019","objectID":"/posts/cbus-mqtt-bridge-on-raspberry-pi/:0:0","series":null,"tags":["IoT","Linux"],"title":"CBus MQTT Bridge on Raspberry PI","uri":"/posts/cbus-mqtt-bridge-on-raspberry-pi/#"},{"categories":null,"content":"Virtual Private Networks are unmissable; however with many states now banning and actively blocking these tunnels the search for an alternative approach is appropriate, if we are to protect our identity and intellectual property. One technology which claims to have fantastic throughput when compared to the stable IPSEC solutions, and based on benchmarks I have calculated leaves OpenVPN protocols in the dust, Wireguard positions itself as an in-kernel VPN solution which is very easy to implement and highly secure. This post will attempt to address the first challenge of implementing the technology, and later we can run our own benchmarks to see how in reality performance measures up. Network Overview The Environment I will implement will contain two endpoints, both of which are directly connected to the public internet. The following table, defines the configuration points of the environment Node Installation Ubuntu 16.04 Azure shell export azResourceGroup=\"USEast_VPNGateway\" export azLocation=\"eastus\" # VNET supports 32 subnets from 192.168.192.0 - 192.168.199.255 export azVnetName=\"USEast_Networks\" export azVnetPrefix=\"192.168.192.0/21\" # Allocating the first /24 subnet for the VPNGateway export azGWSubnetName=\"USEast_VPNGateway\" export azGWSubnetPrefix=\"192.168.192.0/24\" # Define the Gateways WAN Interface export azGWWanNicName=\"USEast_VPNGateway_WAN\" export azGWWanNicPrivateIp=\"192.168.192.4\" export azGWWanNicPublicIpName=\"USEast_VPNGateway_WAN_PublicIP\" export azGWWanNicPublicIpDNS=\"useast-gw\" # Define the Gateway VM export azGWVmName=\"Gateway-USEast\" export azGWVmSize=\"Standard_A1_v2\" export azGWVmUsername=\"damian\" # Create a resource group. az group create \\ --name $azResourceGroup \\ --location $azLocation # Create a virtual network with one subnet az network vnet create \\ --name $azVnetName \\ --resource-group $azResourceGroup \\ --location $azLocation \\ --address-prefix $azVnetPrefix \\ --subnet-name $azGWSubnetName \\ --subnet-prefix $azGWSubnetPrefix # Create a public IP address resource # Append the --allocation-method Static option for the allocation to be static # The DnsName must be unique within the az network public-ip create \\ --name $azGWWanNicPublicIpName \\ --resource-group $azResourceGroup \\ --location $azLocation \\ --dns-name $azGWWanNicPublicIpDNS \\ --allocation-method Static # Create a network interface connected to the VNet # with a static private IP address and associate the # public IP address# resource to the NIC. az network nic create \\ --name $azGWWanNicName \\ --resource-group $azResourceGroup \\ --location $azLocation \\ --subnet $azGWSubnetName \\ --vnet-name $azVnetName \\ --private-ip-address $azGWWanNicPrivateIp \\ --public-ip-address $azGWWanNicPublicIpName # Create a new VM with the NIC az vm create \\ --name $azGWVmName \\ --resource-group $azResourceGroup \\ --location $azLocation \\ --image UbuntuLTS \\ --size $azGWVmSize \\ --nics $azGWWanNicName \\ --authentication-type password \\ --admin-username $azGWVmUsername WireGuard Installation Ubuntu 16.04 Installing is quick and simple, we just need to register the repository, add its content to our catalog index, and install the two packages we require. The following script acomplishs this for us shell apt-get update apt-get upgrade add-apt-repository ppa:wireguard/wireguard apt-get update apt-get install wireguard-dkms wireguard-tools EdgeRouter Download the corresponding package from https://github.com/Lochnair/vyatta-wireguard/releases. You will need to select the version based on your hardware You can choose to upload to the routing, or ssh login routing, sudo su to root , the implementation of similar orders shell curl https://github.com/Lochnair/vyatta-wireguard/releases/download/0.0.20170612-2/wireguard-octeon-0.0.20170612-2.deb -o wireguard-octeon-0.0.20170612-2.deb After the download is complete, you can install the package with the following command shell sudo dpkg -i ./wireguard-ralink-${RELEASE}.deb Configuration Virtual Interfaces Creating the V","date":"5 Oct, 2019","objectID":"/posts/configuration-the-ubiquity-edgerouter-with-wireguard/:0:0","series":null,"tags":["VPN"],"title":"Configuration the Ubiquity EdgeRouter with WireGuard","uri":"/posts/configuration-the-ubiquity-edgerouter-with-wireguard/#"},{"categories":["Infrastructre As Code"],"content":"Microsoft Azure Application Gateway is a Layer 7 application delivery controller (ADC) offered as a service in Azure. It provides load balancing, SSL termination, end-to-end SSL, URL path-based routing, and basic web application firewall (WAF) functionality. Working with the WAF, I usually build a basic configuration in the Portal before exporting the ARM JSON, which, then becomes my primary method to working on this service. Why JSON you may ask… One of my biggest gripes with the Azure Firewall solutions currently is based on thier CRUD (Create, Read, Update and Delete) interface. It always results in a workflow from hell, training along the lines of ‘Edit, Save, WAIT,, Edit, Save, WAIT’ in a painful loop. What should be a fast and straightforward configuration update, typically is a process that must be executed over many hours, preferably on a second screen. Powershell While I spend a large amount of my working time sitting in VS code, with a terminal logged into Azure with both Powershell and Azure CLI, I do not every recall trying to work with the Application Gateway from this interface ever! I was asked to check the HTTP Timeout on one of the Firewalls I have access to, and send the details to a colleague; my first port of call was JSON, and then realized that this is a bit ugly for the request in hand. A quick PowerShell command should sort this out, which of course has even more Idiosyncrasy. Working With Application Gateways in PowerShell Wait for it (Remember my CRUD comment). Well, The PowerShell implementation is restricted by that same API limitations. The first step for updating any existing Gateway is to load the whole gateway configuration from Azure into a PowerShell object. powershell $appGw = Get-AzApplicationGateway -Name p-ap1pub-waf01 From here, all the changes we make are to the PowerShell object $appGw until we are ready to commit the gateway back to Azure with the following command: powershell Set-AzApplicationGateway -ApplicationGateway $appGw Making changes Understanding that all configurations are going to be applied to our in-memory object $appGw; we can use any of the available PowerShell commandlets to alter this object, and once ready to validate we must push these changes back to Azure with the Set-AzApplicationGateway Now you will understand why I ignore this rubbish and work with the ARM JSON! Let’s take a swift tour of working with this object to establish an end-to-end SSL listener, which would be atypical of any implemented configuration. Backend Backend Pool The backend pool for our web servers can be created using FQDNs or IPs; I usually implement this initially using IP addresses. powershell Add-AzApplicationGatewayBackendAddressPool -ApplicationGateway $appGW -Name \"AppPool\" -BackendIPAddresses \"192.168.####101\", \"192.168.1.102\"$appGwBackPool = Get-AzApplicationGatewayBackendAddressPool -ApplicationGateway $appGW -Name \"AppPool\" Backend SSL Encryption Frequently, we establish the connection as a genuine end-to-end encrypted connection; which implies that the Web Application Gateway should have an authentication certificate, so it only forward’s traffic to a backend server if it has the expected SSL certificate. To enable this, we must upload the public certificate used by the backend servers. powershell Add-AzApplicationGatewayAuthenticationCertificate -ApplicationGateway $appGW -Name \"AppPoolPublicCert\" -CertificateFile \".\\myAppPublicCertifcate.cer\"$appGwBackPoolCert = Get-AzureRmApplicationGatewayAuthenticationCertificate -ApplicationGateway $appGW -Name \"AppPoolPublicCert\" Configure the Backend Service Now, we can configure the HTTPS Protocol and authentication certificate the WAF will use to validate the backend servers. powershell Add-AzApplicationGatewayBackendHttpSettings -ApplicationGateway $appGW -Name \"AppPoolHTTPS\" -Port 443 -Protocol Https -CookieBasedAffinity Enabled -AuthenticationCertificates $appGwBackPoolCert$appGwBackPoolHTTPS = Get-AzureRmApplicationGatewayBackendHtt","date":"3 Oct, 2019","objectID":"/posts/configuring-the-web-application-firewall-with-powershell./:0:0","series":null,"tags":["Powershell","WAF"],"title":"Configuring the Web Application Firewall with PowerShell.","uri":"/posts/configuring-the-web-application-firewall-with-powershell./#"},{"categories":null,"content":"Quickly update a new Raspberry Pi, which has an install of Raspbian Buster with Docker and Docker-compose. Docker This is simple, as the Docker team have done all the work shell curl -fsSL get.docker.com -o get-docker.sh sh get-docker.sh And, we can add our user to the Docker group so we do not need the sudo every time. I am using the environment variable $USER; which indicates who is logged in currently. In my case this is the user pi. shell sudo usermod -aG docker $USER Right, that was painful. now reboot the Pi and we are solid. Docker-Compose This is actually a Python script. Raspbian Buster is shipped with Python 3.6; so we just need to add PIP3 to install the python packages from pypy shell sudo apt-get install -y python3 python3-pip sudo pip3 install docker-compose Wow, that was a struggle, lets check we are good shell docker-compose --version ","date":"1 Oct, 2019","objectID":"/posts/installing-docker-and-compose-on-raspbian-buster/:0:0","series":null,"tags":["Docker","Linux"],"title":"Installing Docker and Compose on Raspbian Buster","uri":"/posts/installing-docker-and-compose-on-raspbian-buster/#"},{"categories":null,"content":"There are many projects posted over the web which implement the excellent FastLED library on the ESP12 processor; however locating a project which implements this on the more powerful sibling is a lot more difficult. So, with a few failed attempts and a lot of patching samples together; I have a stable running implementation which you can clone or fork to get up and running quickly with your own projects. The sample includes 2 different sequences, a simple moving dot; and a more colourful Cylon effect. The code is complied within Visual Studio Code; with the Platform.IO environment; and includes a working settings file while will automatically install the required libraries, ready to compile and flash to your device. Check out the repo on GitHub @ https://github.com/DamianFlynn/ESP32FastLED; and if you have issues please use the tracker. Enjoy ","date":"11 Sep, 2019","objectID":"/posts/running-fastled-on-the-dual-core-esp32/:0:0","series":null,"tags":["ESP","IoT"],"title":"Running FastLED on the Dual-Core ESP32","uri":"/posts/running-fastled-on-the-dual-core-esp32/#"},{"categories":null,"content":"Install the Wireguard Package SSH directly to your USG, and run the following commands: shell curl -L https://github.com/Lochnair/vyatta-wireguard/releases/download/0.0.20190123/wireguard-ugw3-0.0.20190702-1.deb -o /tmp/wireguard.deb dpkg -i /tmp/wireguard.deb Create the Tunnel Secrets To keep stuff private, we will encrypt the traffic using a long password, known as a ‘Key’. To make sure this is unique, we will use a tool provided by Wireguard to make a random key for us. shell cd /config/auth umask 077 mkdir wireguard cd wireguard wg genkey \u003e wg_private.key wg pubkey \u003c wg_private.key \u003e wg_public.key Configure the Tunnels While still connected to the USG, we will now create the Interface which will be our end of the tunnel. If we consider this as a Bridge, then as we configure this interface, we will provide the address for our side and also the address of the far side. The far side is protected from just anyone connecting to it by using another long password (key) which we need to know before we can complete this process. In this example 192.168.33.1 is assumed to be your network, you should change these to match your network space. shell # Set the USG into configuration Mode configure # We start, by creating a new Network space for our side of the VPN set interfaces wireguard wg0 address 10.192.10.2/32 # Configure the Port Wireguard will be listening with set interfaces wireguard wg0 listen-port 51820 # Allow this interface to forward the traffic over our tunnel set interfaces wireguard wg0 route-allowed-ips true # Now, we need to tell the interface the address of the far side of the bridge # And also the password to allow us connect set interfaces wireguard wg0 peer \u003cInsert-Public-Key-Of-Peer-Here\u003e endpoint 14.28.207.179:51820 # Now, we will tell the far side of the bridge about our network # This is to ensure that the far side lets our network get out of the tunnel # The sample only allows the IPs 192.168.33.101 to 192.168.33.106 to cross over # you can choose to let everything by using the address 0.0.0.0/0 set interfaces wireguard wg0 peer \u003cInsert-Public-Key-Of-Peer-Here\u003e allowed-ips 10.192.10.0/32 # Lets tell the interface where to find our long password we created earlier set interfaces wireguard wg0 private-key /config/auth/wireguard/wg_private.key # Make the changes active, save them and exit configuration mode commit save exit Firewalls block traffic And our tunnel is no exception, so we need to allow our new Tunnel Interface to be permitted to let the traffic flow. In this case we need to let the far side of the bridge connect back to us; after all there is no point sending traffic over if nothing can come back ! shell # Set the USG into configuration Modeconfigure # Configure the firewall set firewall name WAN_LOCAL rule 20 action accept set firewall name WAN_LOCAL rule 20 protocol udp set firewall name WAN_LOCAL rule 20 description 'WireGuard' set firewall name WAN_LOCAL rule 20 destination port 51820 # Make the changes active, save them and exit configuration mode commit save exit Afterwards dump the config.gateway.json and put it in the controller so it do not get overwritten Reconfigure after an Update Copy your backed up config.gateway.json to /var/lib/unifi/data/sites/default on the system running the Controller (which might also be a Cloud Key). Then through the Controller Web UI navigate to Devices, click on the USG row and then in the Properties window navigate to Config \u003e Manage Device and click Provision. ","date":"17 Aug, 2019","objectID":"/posts/configure-wireguard-on-unifi-usg/:0:0","series":null,"tags":["VPN"],"title":"Configure Wireguard on UniFi USG","uri":"/posts/configure-wireguard-on-unifi-usg/#"},{"categories":null,"content":"I spend the majority of my time working on my Windows machines, and for many scenarios, I find it difficult to complain. However, when Windows decides to dig the boot in and not co-operative; usually is when I grab my Mac Book and get the work done. However, running aware from the problem rarely is a good fix for the issue; My latest battle has been Pester. The testing framework builds on Powershell, and by the grace of God, now shipped as part of the Windows 10 operating system. The issue is that the included version is 3.4 and at the time of writing the current release is 4.6. Typically, this is a non-issue a directly issuing an Update-Module command addresses the issue and allow the product to continue. In the odd case we might need to revert to a push and include the -Force switch; but for various reasons, this sometimes also fails; which is the case in this Pester Module scenario The Hard Way A little research identifies that over time, a few things have evolved with this module. In our case, the certificate used for signing the modules has changed, and for obvious reasons of security, the normal processes are failing. To accomplish the objective in this case, I have reverted to removing the pre-installed version of pester, and then once a memory; I can proceed to deploy the newest release of this product. Using an administrative PowerShell session, I proceed to issue the following commands; which define where the module in question is residing on my system and then assume ownership of the associated files, which I then delete. After all, Powershell modules are discovered, based on the search folder they have been installed within. powershell $module = \"c:\\Program Files\\WindowsPowerShell\\Modules\\Pester\" takeown /F $module /A /Ricacls $module /reset icacls $module /grant Administrators:'F' /inheritance:d /T Remove-Item -Path $Module -Recurse -Force -Confirm:$false Now, we can put the hammer away, and proceed to deploy the module we originally required, this time without challenge. powershell Install-Module -name pester -MinimumVersion 4.3.0 Happy Testing ","date":"29 Jan, 2019","objectID":"/posts/updating-pester-on-windows-10/:0:0","series":null,"tags":["Powershell","Pester"],"title":"Updating Pester on Windows 10","uri":"/posts/updating-pester-on-windows-10/#"},{"categories":null,"content":"With a multitude of Raspberry PI’s deployed around the house, each taking a dedicated duty in ensuring that services run transparently; It is not uncommon for me to discover the initialization scripts designed to have these services auto start at boot is not working. The content of this post is a reference for different methods which can be employed to resolve these stubborn daemons; which always are to fond of reappearing after an unplanned outage; or what is more commonly referred to as a Power Failure! rc.local To start a program on your Linux distribution (I am focusing on Raspbian running on a Raspberry Pi) at start-up, before other services are started, we will use the file rc.local. Editing rc.local On your Pi, using nano or vi which are installed by default, using elevated permissions trough sudo, we will edit the file /etc/rc.local: shell sudo nano /etc/rc.local Add commands to execute the program, using absolute path references of the file location. The final command in the file should be exit 0 to indicate to the OS that we are terminating without error, then save the file and exit. shell ## Start our Node Application sudo node /usr/local/bin/cgateweb/index.js exit Program which are not expected to terminate, (runs continuously in an infinite loop) should be stated as a forked process by adding an ampersand \u0026 to the end of the command. Failure to address this scenario will prevent the OS from completing its boot process. The ampersand allows the command to run in a separate process and continue booting with the main process running. shell sudo node /usr/local/bin/cgateweb/index.js \u0026 Note: A script added to /etc/rc.local is added to the OS boot sequence. A bug here will prevent the OS boot sequence progressing. Recommend that the script’s output and error messages are directed to a text file for debugging. shell sudo node /usr/local/bin/cgateweb/index.js \u0026 \u003e /var/log/myservice.log 2\u003e\u00261 .bashrc The .bashrc file executes on boot and also every time when a new terminal is opened, or when a new SSH connection is made. Normally, we would spawn our program, by placing the command at the bottom of /home/pi/.bashrc file. The program can be aborted with ‘ctrl-c’ while it is running! shell sudo nano /home/pi/.bashrc Add commands to execute the program, using absolute path references of the file location. shell echo Running at boot sudo node /usr/local/bin/cgateweb/index.js \u0026 The echo statement above is used to show that the commands in .bashrc file are executed on bootup as well as connecting to bash console. init.d directory The /etc/init.d directory contains the scripts which are started during the boot process, and also during the shutdown or reboot process. Create a new file in the /etc/init.d directory shell cd /etc/init.d sudo nano sample.py With the following sample content, we define a Linux Standard Base (LSB) (A standard for software system structure, including the filesystem hierarchy used in the Linux operating system) init script. shell # /etc/init.d/sample.py ### BEGIN INIT INFO # Provides: sample.py # Required-Start: $remote_fs $syslog # Required-Stop: $remote_fs $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start daemon at boot time # Description: Enable service provided by daemon. ### END INIT INFO init.d scripts require the above runtime dependencies to be documented so that it is possible to verify the current boot order, the order the boot using these dependencies, and run boot scripts in parallel to speed up the boot process. LSB Init Scripts guide. Finally, the script in the /etc/init.d directory should be executable, and added to the init database with the following commands: shell sudo chmod +x sample.py sudo update-rc.d sample.py defaults SystemD systemd provides a standard process for controlling what programs run when a Linux system boots up. A sample unit file is provided by default in the OS, located at /lib/systemd/system/sample.service shell sudo cp /lib/systemd/syst","date":"14 Jan, 2019","objectID":"/posts/waking-deamons/:0:0","series":null,"tags":["Linux"],"title":"Waking Deamons","uri":"/posts/waking-deamons/#"},{"categories":["Azure Management"],"content":"After years living in tools like Visual Studio, and PowerShell; Currently my primary landing ground is Visual Studio Code. With my target audience firmly defined as Azure; In this post I am going to share my notes on how to get these two tools working harmonisly; and to make the experience a little richer, we will also mount the underlying Cloud Drive File Share of the Azure Cloud Shell on our local computer as a PowerShell Drive (PSDrive). Azure Cloud Shell in Visual Studio Code In VSCode we will naviagte to the Extension icon and search for Azure Account, then install it. The Azure Account extension provides a single Azure sign-in and subscription filtering experience for all other Azure extensions. This extension also exposes the Azure’s Cloud Shell service in VS Code’s integrated terminal. Once the extension in installed you will see the button to Reload your Visual Studio Code instance, which you must complete before we can proceed. Sign In to Azure Now, Open the VS Code command palette (using the key sequence CTRL, SHIFT + P or F1) and select the option to Sign-in to your Azure Subscription; by typing Azure: Sign In Once you click on Sign-in, Visual Studio Code will launch your browser and you will be naviagted to login to your azure account. Select the appropiate account and complete the authentication process. Once authenticated, the browser window will confirm, and tell you to return to VS Code. As a confirmation of your autentication state, in the status bar of VS Code a new element which is prefixed with the word Azure: and postfixed with the account you just authenticated with will be presented. Cloud Shell in VS Code Now that we are signed in, launch the VSCode command palette again (CTRL, SHIFT+P or F1), this time typing Azure: Open PowerShell in Cloud Shell or Azure: Open Bash in Cloud Shell Note: This extension requires Node.JS; If not found on your system, you will be prompted to remediate. VSCode will update the status bar to indicate it is activating the extenstion, If your account is determined to have access to mulitple Azure AD Directories, you will be prompted to select the directory you wish to work in for this session from the drop down list. After a few moments you will then observe the Terminal window launch, and the session will be be entitled as PowerShell in Cloud Shell. powershell Select directory... Requesting a Cloud Shell... Connecting terminal... Welcome to Azure Cloud Shell Type \"az\" to use Azure CLI 2.0 Type \"help\" to learn about Cloud Shell VERBOSE: Authenticating to Azure ... VERBOSE: Building your Azure drive ... Azure:/PS Azure:\\\u003e Mount Azure Cloud Shell drive Locally For the best flexibility you will regularly have the requirement of interacting with the files in the Cloud Drive, directly from your local working environment. Launch your browser, and authenticate to https://portal.azure.com Locate the Resource Group which contains the Cloud Shell Storage Open the Storage Account and select the Files Service Inspect the names of the presented File Shares, and select the name which contains your chosen authentication account, eg cs-technology-damianflynn-com-1001100110011001 Finally, On the File share blade, you will select the ‘Connect’ button, which will present a new blade to the right of the window. In the blade, choose a Drive letter and then copy the auto-generated PowerShell Commands to map your Cloud Drive. In a powershell session on your local machine, paste the copied commands; which will automatically mount the cloud drive to your chosen drive letter on your local computer. From here you can now access and interact with the cloud drive, making changes which will appear real time in the Cloud Shell. Enjoy! ","date":"4 Jan, 2019","objectID":"/posts/configure-vs-code-with-azure-cloud-shell/:0:0","series":null,"tags":["Code"],"title":"Configure VS Code with Azure Cloud Shell","uri":"/posts/configure-vs-code-with-azure-cloud-shell/#"},{"categories":null,"content":"First, allow me to thank all of you that are continuing to read this blog; despite the drought which I am predicting has now passed. I have a long list of topics which I wish to address in detail throughout the year; focused on the areas which I am most passionate. Speaking Last year I had the opportunity to meet a lot of you in various parts of the globe, primarily speaking at some significant events; including the ‘Nordic NIC Conference’, ‘Cloud and Datacenter Conference’, ‘Tech Camp’ and ‘Experts Live’. I am somewhat disappointed to have learned that after 3 years presenting at the NIC Conference, this year I wont be returning, due to concerns of competition (community spirit?) as I am involved with the management of the Azure Track for the new Experts Live Norway event; happening on May 29th in Oslo! I have been accepted to present at some different conferences throughout 2019; most specifics a little later. Azure Working with Azure Infrastructure on a daily basis; I have much to share related to many exciting technologies including Terraform, Azure Resource Manager, DevOps Pipelines, Policies, and Governance IoT After much fun during Christmas, I also plan to share some insights into Home Automation and related technologies; Some of these are foundational, and leverage open source offers including OpenHAB and Home-Assistant; others integrate directly with Azure to leverage the IoT hub and AI services which it has to offer trough standards like MQTT. I also anticipate covering some more device-specific technologies; including a standard from my past DMX512, which has evolved to a new cool standard e1.31. Hint - From Dj Lights to Christmas Lights; but don’t worry; I promise to decrypt is jargon on the way! Roll On 2019 If you have any topics or questions which you believe I should be able to help explain, or should be considered as topics for some posts, presentations or webinars; then please do let me know; either via mail, twitter or comments on this blog. Slainte! ","date":"2 Jan, 2019","objectID":"/posts/welcome-to-2019/:0:0","series":null,"tags":null,"title":"Welcome to 2019","uri":"/posts/welcome-to-2019/#"},{"categories":null,"content":"Assuringly I am not alone, when we sit down as a family and talk about our day, and the plan for the next days or the weekend; only to realise that we have some real scheduling issues; because you totally forgot that you would take the children to an event; while your partner had a long-standing appointment with the hairdresser. Letting this happen once or twice, is forgivable, but happing on a regular basis; is the recipe for a lousy dispute; that we do not need. Synchronising Schedules The solution, of course, is simple, We need to share some visibility of our schedules, and of course be consistent in making sure that we record these events in the calendar in the first place. The problem is, however, what calendar do we use, and NO; a whiteboard stuck to the fridge or some other silly place is not an option. This is a digital era, and I need a digital solution; as I can not predict where I might be when I agree with that business trip, or customer call which stomps all over that crucial other thing that I have now entirely forgotten about, and its not possible to run from Oslo to my Fridge door to check! Environment Survey First, I need to determine what are the artefacts I am dealing with here; The Actors My Wife Her Schedule Myself My Work Schedule, including Trips, Workshops and Meetings My MVP Schedule, including Product Calls, Meetups, Conferences, Community Time 2 Children Pre-School, Appointments, etc. Family Unit Outings, Appointments, Events, Games, etc. The Actions Now, Let’s consider the unconscious actions we take My Wife iPhone to update her Calendar Myself Outlook primarily to manage both Work and MVP schedules Google/Alexa/Siri Shout at these devices to update the Family calendar The Objective The vision is simple: The approach Family Calendar Using the shared Family calendar features offered by many of the consumer-focused cloud offerings; leverage these services, so that shared appointments are visible; while managing out own independent personal scheduled. While both my wife and I are currently iPhone users; there is no guarantee that this will always be the case; especially looking at the inflation on the newest models. Accessing the iCloud Calendars outside the Apple ecosystem is not a fantastic experience. Combined with the desire to leverage Siri, Alexa and Google Assistant; my current conclusion is that the best-supported Calendar for families, for free is Google Calendar. Therefore with existing Gmail accounts, we established a Family relationship and gained the shared family calendar feature. Additionally, we Invited each other to our personal calendars so we can see the potential conflicts which may arise My Work Calendars Others essentially manage my Work and MVP Calendars; as they set meetings, appointments and so on, which I usually am obliged to join. I have kept these two environments independent; mainly due to the NDA’s which I have signed which results in lots of sensitive emails flowing which I am not comfortable being managed by other mail administrators (I wore that hat long enough to understand the potential access available) Using Microsoft Flow, I created three flows to synchronise these schedules Sync Work calendar to my Personal Google Calendar Sync MVP calendar to my Personal Google Calendar Sync Family Google Calendar appointments to Work Calendar [as Time Blockers] Sync MVP calendar to my Work Calendar [as Time Blockers] The Big Picture Now with the boundaries defined, and the flows described, let’s visualise this challenge graph TD subgraph Google Calanders C[My Schedule My Personal Google Calander] G[Wifes Schedule Her Personal Google Calander] D(Family Shared Family Google Calander) end subgraph My Calanders A[MVP Calander Office 365] B[Work Calander Office 365] end E(My Assistants Profile Google/Alexa/Siri) F(Wife Assistants Profile Google/Alexa/Siri) A -.-\u003e|Flow MVP Blockers to Work|B A --\u003e|Flow MVP to My Schedule|C B --\u003e|Flow Work to My Schedule|C C -.- D D --\u003e|Flow Family Bl","date":"12 Dec, 2018","objectID":"/posts/fixing-calendar-chaos/:0:0","series":null,"tags":null,"title":"Fixing Calendar Chaos","uri":"/posts/fixing-calendar-chaos/#"},{"categories":null,"content":"In a previous post, I referred to an embedded device which is called a NodeMCU. This device is a developer kit, designed to make it easy to develop and test programs for the embedded ESP8266 System on a Chip. Many manufacturers are offering both developer and production kits which leverage this SoC; including the Wemos D1, Lolin, and in my case the NodeMCU. The function of these developer kits is to add some supporting features, for example, a USB to serial converter which makes programming a lot easier, in addition to a couple of buttons, and maybe an LED or two, for testing some simple scenarios with. graph TD A[ESP8266] --\u003e|Hosts| B[ESP12] B --\u003e|Encapsulates| C[NodeMCU] Once your development efforts are at a stage ready for production, we can implement the ESP12 module on custom circuit boards, designed for the specific scenarios you might be addressing. The cost of the models is meager and can be significantly reduced further based on the order size. For a tiny quantity of 10 unit’s, we will have change out of €15, after Postage and Packing. The developer board, is a little more expensive, for example you can grab these from many online suppliers including my link to Amazon, which offers two developer boards for €8, seriously not going to burst any banks here. Some History So why am I playing with these devices? I have been working on embedded technologies for over 30 years; starting with my first industrial deploying in a rubber injection moulding company, using a board from an American company called Tern, Inc; developing the code in C, and running my own communications protocol across an RS485 network! This was way back in 1994! Now, I am showing my age! The scenario at the time was to monitor the state of the Moulding press, ensure the safety guards were down while injection was happening, reporting temperature which has been managed by a Siemens PLC, and then using a digital scale, counting the number of rubber parts which were just created and placed in a plastic box. 30 years later, not a lot has changed, we still use conveyor belts, PLCs, digital scales, thermocouples, and so on; what has changed however… The computers have shrunk considerably Protocols are now published and supported with free open-source libraries Projects are shared so development can be accelerated, and new patterns and practices discovered and learnt Costs have decreased, while reliability has increased. Data is preferred to be stored in large pool’s; exposing some amazing value attributes. Learning My original deployment failed after a few weeks in live production, for reasons which are now very obvious, but at the time were not so transparent. Considering the environment, all the heavy 3-Phase machinery introduced a lot of noise, Not noise as in you should wear ear protection, but electrical noise, Spikes, and drops, which on a network of 2 twisted wires with no error handling; and a rudimentary communications protocol with nothing more than a CRC code for data blocks, resulted in some false data getting stored, or worse lost. graph TD S1[Thermocouple] S2[Light Curtain] S3[Ram Position] S4[Injector Pressure] P1[PLC] C1[Datalogger] N1[RS485 Network] O1[Optic Tx and Rx] O2[Optic Tx and Rx] P2[Data Aggregator] B1[Business Presentation] E1[Enterprise Resource Planner] subgraph Moulding Press S1 --\u003e P1 S2 --\u003e P1 S3 --\u003e P1 S4 --\u003e P1 subgraph Datalogger P1 --\u003e C1 S1 -.- C1 end end P1 --\u003e N1 N1 --\u003e O1 N1 --\u003e O2 O1 --\u003e P2 O2 --\u003e P2 P2 --\u003e B1 P2 --\u003e E1 The fix at the time was costly. Firstly, the network required to be insulated; trough both software (firmware) updates to harden the protocol, and better error handling; along with physical shielding from the noise base; In the worst areas of the factory, the copper RS485 network had to be redeployed with optical cables, which transmitted the signal using couplers and decouplers. Very cool stuff back then. graph LR S1[Thermocouple] S2[Light Curtain] S3[Ram Position] S4[Injector Pressure] P1[PLC] C1[Data","date":"6 Dec, 2018","objectID":"/posts/my-journey-to-the-internet-of-things/:0:0","series":null,"tags":["IoT"],"title":"My Journey to the Internet of Things","uri":"/posts/my-journey-to-the-internet-of-things/#"},{"categories":["Books"],"content":"Careers are what we all invest our energy and emotions in, either positively or negatively. Positive being the belief that we can make significant progress in this or another organisation and that it will give us the wherewithal to have a happy and productive life. Negative in that we can feel that everyone is out to get us, and the slippery pole has been freshly greased to scupper us. That it’s a “not what you know but who you know” world, and, unfortunately, you don’t know anyone. **ISBN-10 ‏ : ‎ **1463756682 Timothy Dyer Wavemaker The story starts with a young surfer who has a simple dream to get his dad off his back. This simple dream ended up changing the lives of everybody he touched. Being an only child and not having a mother in his life, Glenn Davenport’s struggle between his old ways of coping with his overbearing abusive father, Jeff, and a new way he planned to cope with his dad’s unpredictable ways. As a teenager who had just started surfing he feels crippled in his mind by a frustrated underachieving drunk of a father who works in a marina on boats. Glenn changes his mode of reactionary defensive lies and deceitful ways, to a truthful loving compassionate one. Fifteen years old he starts this change of belief with the help of a guru, Michael, who miraculously materialized at the moment of a life and death crisis for this young man. This story is based on facts with fiction interwoven throughout it. Experience the surfing gang who he fell in with willingly. They all worked really hard at having as much free fun as they could get their adolescent hands on. Through the use of Glenn’s passion for surfing and everything that comes with it, we follow his growth in remembering the divine nature within himself that is spurred on by Michael. Taste the flavor of the beach life that engages all the senses in this story that draws you into Glenn’s world. Having faith in getting Jeff involved in Glenn’s love of the ocean, Michael planted a seed in Glenn’s mind. With the help of his surfing gang they tried their hand at making a wave in the ocean by tricking his dad into captaining this project with his own love of the sea. This incredible journey that this teenager travels on, changes him from a lesser-than spirit into a stand-up self respecting man. As Glenn grew in confidence and self awareness, Michael revealed to him more and more of the mystical powers that are within all of us. Traveling half way around the world to Japan, Europe, and trips from the U.S. to Mexico, Michael’s teachings freed Glenn’s beliefs from the fears of his past. We read about the struggles he found himself in, on the beaches, the massive waves he rode, and the boats on and in the water. The destitute in Mexico, Mexican officials, Federales, and the mission with Father Juan, as the head of the sanctuary, progressively propel Glenn into a world of selfless service for the local people that have no hope or dreams left in their lives. Going with what he discovered in his heart, his compassion drove him to a life so magical that only a few that have the faith and courage to walk on this path ever achieve. The characters he found himself involved with along the way only heighten his love of humanity which emboldens him to greater and greater accomplishments. This amazing story will challenge you to strive for the same essence that Glenn found after conquering his own fears. Willing himself to walk into the unknown, proving he has what it takes day after day he was included into Michael’s world. For his efforts he was allowed to become a part of a selfless order of spirits that relieve the misery in this wonderful world of ours. This order was affectionately nicknamed by some as the Wind Walkers. With this change of mind heaven could not be any sweeter than what he discovered within, then, throughout the world. To find yourself on the doorstep of hell in your own mind one moment, then finding yourself cheering on eternity the next is something to read abo","date":"27 Nov, 2018","objectID":"/posts/wavemaker/:0:0","series":null,"tags":null,"title":"Wavemaker","uri":"/posts/wavemaker/#"},{"categories":["Azure Management"],"content":"My colleagues and friends Tao Yang , and Stanislav Zhelyazkov have both recently posts interesting topics on how to implement your Azure Policy as Code which I strongly recommend you take a few moments to review Using ARM Templates to deploy azure policy definitions that require input parameters Defining input parameters for policy definitions in ARM Templates Improving Readability Both of these topics address the core of the challenges we face when approaching policy as an Infrastructure as Code problem. However, one of the things that is lost in the translation is the readability of the templates which they are deploying. Not to reinvent the wheel, I am going to use the same template which Stan presented in his post, and make a small tweak to the process which he has employed to deal with the ' single quote problem! ARM follows most of the standard JSON escape sequences, therefore the following examples are quite useful Escaping a Single quote Azure ARM behaves nicely with a simply doubling the single quote characters; just as we apply in Visual Basic. [concat('This is a ''quoted'' word.')] which then provides the output of This is a 'quoted' word. Escaping a Double quote For the Double quotes, we use the normal escape character /. [concat('''single'' and \\\"double\\\" quotes.')] will render the output as follows 'single' and \"double\" quotes. The Solution With this simple trick, we can replace the variables definition which the guys used with the following snippet: json \"variables\": { \"filterVNetId\": \"[ concat( '[concat(parameters(''virtualNetworkId''),''*'')]' ) ]\" } I am sure that this is a lot easier to read, and therefor debug; So the full template would look as follows json { \"$schema\": \"https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"vNetId\": { \"type\": \"string\", \"defaultValue\": \"ScreenConnect\" } }, \"variables\": { \"filterVNetId\": \"[ concat( '[concat(parameters(''virtualNetworkId''),''*'')]' ) ]\" }, \"resources\": [ { \"name\": \"vm-creation-in-approved-vnet-definition\", \"type\": \"Microsoft.Authorization/policyDefinitions\", \"apiVersion\": \"2018-03-01\", \"properties\": { \"displayName\": \"Use approved vNet for VM network interfaces\", \"policyType\": \"Custom\", \"mode\": \"All\", \"description\": \"Use approved vNet for VM network interfaces\", \"metadata\": { \"category\": \"IaaS\" }, \"parameters\": { \"virtualNetworkId\": { \"type\": \"string\", \"metadata\": { \"description\": \"Resource Id for the vNet\", \"displayName\": \"vNet Id\" } } }, \"policyRule\": { \"if\": { \"allOf\": [ { \"field\": \"type\", \"equals\": \"Microsoft.Network/networkInterfaces\" }, { \"not\": { \"field\": \"Microsoft.Network/networkInterfaces/ipconfigurations[*].subnet.id\", \"like\": \"[variables('filterVNetId')]\" } } ] }, \"then\": { \"effect\": \"deny\" } } } }, { \"name\": \"vm-creation-in-approved-vnet-assignment\", \"type\": \"Microsoft.Authorization/policyAssignments\", \"apiVersion\": \"2018-03-01\", \"dependsOn\": [ \"[resourceId('Microsoft.Authorization/policyDefinitions/', 'vm-creation-in-approved-vnet-definition')]\" ], \"properties\": { \"displayName\": \"Use approved vNet for VM network interfaces\", \"description\": \"Use approved vNet for VM network interfaces\", \"metadata\": { \"assignedBy\": \"Admin\" }, \"scope\": \"[subscription().id]\", \"policyDefinitionId\": \"[resourceId('Microsoft.Authorization/policyDefinitions', 'vm-creation-in-approved-vnet-definition')]\", \"parameters\": { \"virtualNetworkId\": { \"value\": \"[parameters('vNetId')]\" } } } } ], \"outputs\": {} } Summary Using native escape sequences in ARM assist in the overall readability of the final code. Thank you Tao and Stan for the inspiration ","date":"20 Nov, 2018","objectID":"/posts/defining-policy-as-code-with-arm-templates/:0:0","series":null,"tags":["ARM","Policy","Governance"],"title":"Defining Policy as Code with ARM Templates","uri":"/posts/defining-policy-as-code-with-arm-templates/#"},{"categories":["Azure Edge Devices"],"content":"For the past year, I have been using a tiny board, known as the NodeMCU which is essentially a developer board for a module know as the ESP8266. The NodeMCU is formed by an ESP12E, which still has an ESP8266EX inside it. This device is really nice to work with, it is supplied preconfigured with a Micro USB input, for both programming and power. The term NodeMCU usually refers to the firmware, while the board is called Devkit. NodeMCU Devkit 1.0 consists of an ESP-12E on a board, along with a voltage regulator, a USB interface. ESP-12E The board created by AI-THINKER, which consists of an Espressif ESP8266EX inside the metal cover. The microchip has a low-power consumption profile, integrated WiFi and the RISC Tensilica L106 32bit Processor has a maximum clock of 160 MHz The following illustrates the pinouts on this board ESP-12E_Pinout IDE I have so far being developing on this board using VS Code, and its integrations with the Arduino IDE. While this works well, I am currently considering alternative approach’s; as the debugging experience is far from optimal in my opinion; but that is work for another day. The power of this board however, is understanding how to connect with outside world, and in this case what are the correct pin identifiers, trough reference of the NodeMCU datasheet and how the board boot’s. NodeMCU Pinout The Devkit board which we are leveraging maps the pinouts from the ESP-12E module as follows: Translating this however to the code we are going to develop in the Arduino sketch’s; we need to reference the pins with thier respective names; which is illustrated in this following image Use the number that is in front of the GPIO or the constants as follows Pin IO Functions When performing INPUT and OUTPUT tests on the pins, we obtained the following results: Flashing Sketch The following simple sketch should flash an LED connected directly to the NodeMCU c //Connect a testing LED to GPIO14 which is pin D5 #define LED D5 void setup() { pinMode(LED, OUTPUT); } void loop() { digitalWrite(LED, HIGH); // Turn on the LED delay(1000); // Wait 1 Second digitalWrite(LED, LOW); // Turn off the LED delay(1000); // Wait 1 Second } Using a very simple hook up example. Alternatively, without any external LED, we can set the LED to use the onboard LED with the mapping to D0 or the constant LED_BUILTIN Important: Please note that there are lots of generic ESP8266 boards and there is the possibility that some of them are sold under the name of NodeMCU and have different pin mappings. Besides that, there are different NodeMCU versions. ","date":"18 Nov, 2018","objectID":"/posts/nodemcu-pinouts/:0:0","series":null,"tags":["ESP"],"title":"NodeMCU Pinouts","uri":"/posts/nodemcu-pinouts/#"},{"categories":null,"content":"At this point we are almost ready to go live with our site, however, one of the cornerstones to growing and sharing is communication. Wordpress In the world of Wordpress, this was a standard core feature, which leveraged the fact that the pages were rendered on demand from a backend database. In this scenario, the same approach is offered to maintain a commenting platform. However, as I noted earlier; given that Wordpress powers a very large portion of the blogging surface of the internet; it is an obvious target for hacking, just refer to the CVS database for a glimpse of what this looks like in reality. I have had more than over of these exceptions result in defacement or excessive spam in the comment system. The real objective, however, is to bloat the database which will then result in the site going offline as the database reaches its maximum limit based on your host or plan. Recovering from this mess is slow and painful, and you must also not ignore the fact that you now should also update the runtime; a process we try to ignore as this exercise generally results in breaking extensions and taking the site offline for a little time. Static Sites So, if we do not have the luxury of a database to host our comments in the static site configuration (recall all we have is HTML and client-side JavaScript); how on earth do we implement this critical feature. Of course, we can use the cloud! There are many SaaS offerings which are designed to integrate into our site but offload all the storage and processing to the service Additionally, many of these are free to use, if you agree to let the service display a couple of advertisements. Previous I have used services from Disqus on my site to offload the challenges of hosting and keeping updated my own. Disqus Out! As I coded the liquid for this side I also implemented Disqus as the commentary service. However, immediately after turning this on for my posts the page load time was almost 3 times slower! Adding insult to injury the adverts have evolved to be click bate and not relevant to my content what so ever. Disqus does offer a Not Free option which addressed the Advertising a bit better, but that does not explain why the massive performance hit? Tracing my site loading time with Chromes F12 development tools expose the shocking truth. Adding Disqus to the site results in over 50 treads to tracking and other undesirable sites Therefore I immediately deleted the liquid code and stopped any further integration of this service. It’s gone and good ridden Utterance.es Watching how Microsoft recently replaced their commentary service on the docs.microsoft.com sites to leverage GitHub, I decided that this might be a really good solution for this site also. After a little research, I found a lovely match called utteranc.es which requires that you log in with your Github account, and will create a new issue per post in my site, that can be tracked and managed as normal issues within github which is pretty awesome. (I assume based on my content and audience that this should not be a problem - let me know on Twitter if I am wrong about this) Implementing Utteranc.es Adding this feature is trivial, Really trivial! We require a public Github Repo Authorise the [Utteranc.es Bot][https://github.com/apps/utterances] access to the selected Repo Add the following javascript code to our page, updating the paramater repo=\"[ENTER REPO HERE]\" to match the Repo name; for example repo=\"[damianflynn/damianflynn.github.io\" javascript \u003cscript src=\"https://utteranc.es/client.js\" repo=\"[ENTER REPO HERE]\" issue-term=\"pathname\" theme=\"github-light\" crossorigin=\"anonymous\" async\u003e \u003c/script\u003e I have added a little extra logic to determine which pages to offer comments; for example; I do not need this feature on the main landing page. Summary Now, I really want you to tell me what your thoughts about this for a solution? Go On, Leave a comment, even if its just a thumbs up or down! ","date":"30 Oct, 2018","objectID":"/posts/implementing-comments-on-a-static-site/:0:0","series":null,"tags":["SSG"],"title":"Implementing Comments on a Static Site","uri":"/posts/implementing-comments-on-a-static-site/#"},{"categories":null,"content":"With the heavy lifting done in creating the site building mechanics and a solid foundation to build and share upon; our final objective is to automate the process of connecting these two stages. Release Pipeline Technically the goal we are speaking about is the Release Pipeline which will take the artefact (our site .ZIP file) that we created in the Build Pipeline in our previous topic Constructing a new Home with Jekyll and Azure DevOps; and publish this to our storage account. In simple terms, we are going to Unzip the archive to the storage, which is configured to expose the content as a website, and to increase performance we are leveraging a content delivery network. Copying Content As with our Build Pipeline there are a few different options on how to accomplish the work. Currently Azure DevOps is not exposing the release pipelines in an Infrastructure as Code configuration so we will complete this in the portal. We begin with a new Release Pipeline, add add the task Azure File Copy, and set the settings similar to the following: Done!, Seriously; pretty easy right! Replace Content One small issue with the previous approach is that it merely overwrites the content in the blob with the latest version, but it failed to remove any content which might have been removed from the site. Currently, there is no equivalent to the magic XCOPY command that will sync the blob removing the data that is no longer required; that’s a project for another day. The quick and dirty fix for this scenario is first to delete the existing content and then deploy the latest version. This would typically result in a potential outage as the content vanishes for a little time; however, we do not have that concern, because we have decided to use a Content Delivery Network which caches our site. Task 1 Task 2 Tiggers Ready The last point to consider is When should this deployment happen, and that’s also pretty obvious when we think about it. Every time we have a good build of the site, we should check to see which branch just completed the process, and update the relevant site based on this information. Summary Which flow you choose to implement are entirely at your discretion; As I am currently doing much work on the taxonomy of the site; I have implemented the second option of delete and redeploy; but I do in the plan to come back and optimize this by adding a sync process which will be far more efficient. ","date":"24 Oct, 2018","objectID":"/posts/using-pipelines-for-flow-static-site-content-between-markdown-and-foundation/:0:0","series":null,"tags":["SSG"],"title":"Using pipelines for flow static site content between markdown and foundation","uri":"/posts/using-pipelines-for-flow-static-site-content-between-markdown-and-foundation/#"},{"categories":null,"content":"Hosting my site on Wordpress was not super complex; I leveraged the Azure PaaS Services for Web Apps, and orginally the 3rd party support for hosted MySQL database’s. Once I was up and running I quickly realised that all media hosted on the site were landing on the webserver, so a plugin from its marketplace offered the ability to relocate the media to an Azure Blob; offloading some of the challanges. Hosting this was not free, while I could have leveraged the Free Webserver option it did not take a lot of load for this to be causing some unacceptable performance issues; which when combined with a hosted MySQL service which also was not going to be super fast and was limited in the database size; which became event more of a problem when an attack on the site would result in 1000’s of useless comments filling the database to capacity and taking the site down as a direct result. Static Site Hosting Not a lot has to change as we move to the static side model; The web server is still needed to host the side; but the database component is no longer a concern with this approach. However, The cloud is a beautiful thing, and there are so many more ways to reach you goal, and while we are focused, we can complete a lot more for a lot less! For this site I have chosen to run a lean cost model, while providing a super responsive experience to you, my readers and subscribers. Blob Storage Foundations Using the standard Azure Blob Storage offering, I simply copy over the generated HTML site to the account. Assuming the blob is exposed to the public its content is available as a HTTPS endpoint; which simply put is a website. But, alone this is not enough; why? because how would I address requested for pages which do not exist for example; I would not want an ugly HTTP 404 error page to be rendered, but instead a nice response to offer a search, navigation, or other more professional experience. json { \"$schema\": \"https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"vNetId\": { \"type\": \"string\", \"defaultValue\": \"ScreenConnect\" } }, \"variables\": { \"filterVNetId\": \"[ concat( '[concat(parameters(''virtualNetworkId''),''*'')]' ) ]\" }, \"resources\": [ { \"name\": \"vm-creation-in-approved-vnet-definition\", \"type\": \"Microsoft.Authorization/policyDefinitions\", \"apiVersion\": \"2018-03-01\", \"properties\": { \"displayName\": \"Use approved vNet for VM network interfaces\", \"policyType\": \"Custom\", \"mode\": \"All\", \"description\": \"Use approved vNet for VM network interfaces\", \"metadata\": { \"category\": \"IaaS\" }, \"parameters\": { \"virtualNetworkId\": { \"type\": \"string\", \"metadata\": { \"description\": \"Resource Id for the vNet\", \"displayName\": \"vNet Id\" } } }, \"policyRule\": { \"if\": { \"allOf\": [ { \"field\": \"type\", \"equals\": \"Microsoft.Network/networkInterfaces\" }, { \"not\": { \"field\": \"Microsoft.Network/networkInterfaces/ipconfigurations[*].subnet.id\", \"like\": \"[variables('filterVNetId')]\" } } ] }, \"then\": { \"effect\": \"deny\" } } } }, { \"name\": \"vm-creation-in-approved-vnet-assignment\", \"type\": \"Microsoft.Authorization/policyAssignments\", \"apiVersion\": \"2018-03-01\", \"dependsOn\": [ \"[resourceId('Microsoft.Authorization/policyDefinitions/', 'vm-creation-in-approved-vnet-definition')]\" ], \"properties\": { \"displayName\": \"Use approved vNet for VM network interfaces\", \"description\": \"Use approved vNet for VM network interfaces\", \"metadata\": { \"assignedBy\": \"Admin\" }, \"scope\": \"[subscription().id]\", \"policyDefinitionId\": \"[resourceId('Microsoft.Authorization/policyDefinitions', 'vm-creation-in-approved-vnet-definition')]\", \"parameters\": { \"virtualNetworkId\": { \"value\": \"[parameters('vNetId')]\" } } } } ], \"outputs\": {} } To achieve this I need some HTTP routing options House Warming, Inviting Guests It is not hard to find a list of potential solutions in Azure to fill this role; We could use a WebApp, API Managment, or event Azure Functions, or the Azure Functions proxy features. However, we do not","date":"20 Oct, 2018","objectID":"/posts/laying-the-foundation-in-azure-for-a-static-site/:0:0","series":null,"tags":["SSG"],"title":"Laying the foundation in Azure for a Static Site","uri":"/posts/laying-the-foundation-in-azure-for-a-static-site/#"},{"categories":null,"content":"One of the unspoken truths behind the lack of posts in recent history was due to a few bugs, which in the end resulted in an experience where from home it appeared that any new content was published and working; but outside this fortress in the real world, there was a large silence echoing. I really only discovered this issue in May of this year, and was, to say the least, a little agitated with the situation and decided then to change the approach to how I save my notes and share my thoughts. Jekyll After a lot of hours hacking at CSS and JS, neither of which are my strongest points; combined with a whole lot of liquid scripting, which is based on the Python Jinja library; I chose to leverage the open source Jekyll project. This is not to say, that I might not reconsider this again as I am pretty intrigued also with Hugo; but one point is for sure… My days struggling with Wordpress are history. Don’t get me wrong, Wordpress is great, even fantastic, but when it breaks, or its hacked (and boy have I been hacked), or when the comments system becomes a spam target; then its a total nightmare to have to deal with. I want something that is easy to use, a lot less prone to hacking, and painless to host; so my choice was clear from the start - I was going to use a Static Site Generator Building Leveraging GIT for my version control, I have a simple pipeline which rebuilds a new version of the site each time a new commit is made to the repository. I do like to tweak and have actually no less than two approaches to the effort graph LR A[Blog Repository] B[Build Pipeline] C[Docker Based Build] D[Native Build] E[Publish Built Site] A --\u003e|Git Push Trigger| B B --\u003e C B --\u003e D C --\u003e E D --\u003e E As I spend the majority of my time focused on Microsoft Technology stack, I am leveraging Azure DevOps to run my build process; however, if you prefer other tools, for example, Jenkins, CircleCI, etc then the concepts should be easily transportable, as there is nothing truly complex happening at this point. Docker Build Pipeline This version of the pipeline is my favourite, as I can use the same commands on my workstation to run a local web server to watch in realtime what my edits are going to look like when I finally commit, with 100% confidence that there will be no drift, as I use the exact same container for both roles, development and deployment The pipeline I am sharing is in YAML format, which we are going to see a whole lot most of over time, and by sharing this you can easily recreate your own build pipeline with nothing more than a good paste! The build is running on a hosted Ubuntu 1604 instance, but this could be easily replaced with a dedicated build node; however for the amount of time I will use for the building, I should fall well inside the free monthly allocation offered in Azure DevOps; so, for now, this is perfect. The pipeline has only 3 steps graph TD A[Retrieve the relevant commit from Git Repo] B[Run Docker Image to Build Site] C[Move Generated HTML Site to Staging Area] D[Publish Built Site] A --\u003e B B --\u003e C C --\u003e D The YAML representation of the flow is as follows; you can also choose to add the steps in the UX and provide the data below into the relevant fields, as there is a 1:1 relationship between the UX and the YAML Infrastructure as Code yaml resources: - repo: self queue: name: Hosted Ubuntu 1604 steps: - task: Docker@1 displayName: 'Run an image' inputs: containerregistrytype: 'Container Registry' command: 'Run an image' imageName: 'jekyll/builder:latest' qualifyImageName: false volumes: | $(Build.SourcesDirectory):/srv/jekyll $(Build.BinariesDirectory):/srv/jekyll/_site workingDirectory: '$(Build.SourcesDirectory):/srv/jekyll' containerCommand: 'jekyll build --future' runInBackground: false - task: CopyFiles@2 displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory)' inputs: SourceFolder: '$(Build.BinariesDirectory)' TargetFolder: '$(Build.ArtifactStagingDirectory)' - task: PublishBuildArtifacts@1 displayName: '","date":"10 Oct, 2018","objectID":"/posts/constructing-a-new-home-with-jekyll-and-azure-devops/:0:0","series":null,"tags":["SSG","Jekyll","DevOps"],"title":"Constructing a new Home with Jekyll and Azure DevOps","uri":"/posts/constructing-a-new-home-with-jekyll-and-azure-devops/#"},{"categories":null,"content":"10 Years, It is hard to believe that I have been posting thoughts here that long. And how so much has changed since I begun? I started this journey with the encouragement of some amazing people in Microsoft, as an opportunity to spread the news about Hyper-V and even more relevant at the time System Center Virtual Machine Manager which was still known by its code name!. My daily experience with this application, Windows Server, and real-world enterprise issues; positioned me at one of the leading edges of Microsoft Technologies; and fully armed with a true business driver pushing forward. Wounds and pains exposed, I gained a lot of insight to the digital plumbing of these technologies and as a result of a lot of fantastic information to share - sometimes not good news; but never the less - reality. I have primarily worked in the mindset that when I find an issue to be addressed, before sharing, escalating or attacking - I need to stop and consider solutions; which normally result in a more constructive and progressive approach to unblocking my path. That ethos spans back to my days working as what would be considered today in 2018 as an IoT architect; but 20 years ago in a Rubber Molding plant, The Operations Manager always reminded me as I entered his office, “If you don’t have some suggestion for a solution before entering and presenting a problem, leave now, and come back when I am prepared (But don’t spend all day - Problems cost money!).” In Hindsight, this approach challenged my limits every day; but I now also realise that he actually had no technical knowledge, and without my suggestions, we were heading the route of the Titanic! System Center While the solution itself continues to live on, as clear from the very recent launch of System Center 2019 at the Ignite Conference in Florida; My own passion and engagement with this technology has ultimately diminished to a point of history., despite co-authoring and technically reviewing a number of books, speaking at so many events, and investing 1000’s of hours. Personal Redevelopment After almost 20 years I changed Jobs, A decision which was extremely difficult to make; and honestly post that change point; I took at least 6 more months to adjust to the new world order. I found myself amidst a team of like-minded peers, left to find a niche which I could own. Despite working with fantastic scenarios, these new challenges were amazing; and I was learning new stuff again. But, yet I still felt uninspired. Just reflect on the number of blog posts I have published in the last 3 years. My personal life also took a major change; and today after celebrating 21 years of marriage; I am a super proud father of two amazing girls, with my oldest just after celebrating her 4 birthday and the youngest just turned 2. When I reflect on these massive changes, it is a totally different world from when I stood just 10 years ago. Inspiration Last week I participated in my first Microsoft Ignite event; and spent the vast majority of this opportunity meeting with so many old friends who have also evolved into completely new roles within their organizations. As an example, Mr Taylor Brown; I had the honour of meeting Taylor for the first time almost 12 years ago. Back then we both were working on Test Scenarios for Hyper-V; in his role, he ran the labs for Microsoft’s internal testing; and I was responsible for our internal Technology Adoption Program (TAP) Pilot testing. Today, Taylor owns the Docker (Container) features in Windows Server. An amazing achievement, from an inspiring person and a good friend. There are so many amazing people, with just as amazing stories; and I was so proud to be able to stop, and say hello to these icons, and learn how their lives have also changed. Governance As the adoption, and practices of Cloud become centrally focused for so many organizations the focus shifts left, as Compliance, Control, and Culture changes ignite to enable a completely fresh view of the po","date":"5 Oct, 2018","objectID":"/posts/hitting-reset/:0:0","series":null,"tags":null,"title":"Hitting Reset","uri":"/posts/hitting-reset/#"}]